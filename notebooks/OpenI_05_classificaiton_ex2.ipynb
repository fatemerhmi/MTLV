{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, label_ranking_average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task heads\n",
    "In this experiment, we combine different classes and group them in different heads. We then experiment to check if we are getting any better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path,split_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    cols = df.columns\n",
    "    label_cols = list(cols[6:])\n",
    "    \n",
    "\n",
    "    train_df = pd.read_csv(f\"{split_path}/train.csv\")\n",
    "    # convert str to list\n",
    "    train_df['labels'] = train_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    test_df = pd.read_csv(f\"{split_path}/test.csv\")\n",
    "    test_df['labels'] = test_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    val_df = pd.read_csv(f\"{split_path}/val.csv\")\n",
    "    val_df['labels'] = val_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    return train_df, test_df, val_df, label_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create a column for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>head_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heart size is normal and lungs are clear. No p...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiomediastinal silhouette is normal. Pulmon...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardiomegaly. Left lung clear. Large right eff...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal cardiac size and contour unremarkable m...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The cardiac and mediastinal contours are withi...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Frontal and lateral views of the chest with ov...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>Lungs are clear. Heart size normal. No pneumot...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Cardiomediastinal contour and pulmonary vascul...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 0, 0, 1], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>The lungs are clear bilaterally. Specifically,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>Heart size is normal. Lungs are clear. No pneu...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Heart size is normal and lungs are clear. No p...   \n",
       "1     Cardiomediastinal silhouette is normal. Pulmon...   \n",
       "2     Cardiomegaly. Left lung clear. Large right eff...   \n",
       "3     Normal cardiac size and contour unremarkable m...   \n",
       "4     The cardiac and mediastinal contours are withi...   \n",
       "...                                                 ...   \n",
       "1011  Frontal and lateral views of the chest with ov...   \n",
       "1012  Lungs are clear. Heart size normal. No pneumot...   \n",
       "1013  Cardiomediastinal contour and pulmonary vascul...   \n",
       "1014  The lungs are clear bilaterally. Specifically,...   \n",
       "1015  Heart size is normal. Lungs are clear. No pneu...   \n",
       "\n",
       "                                 labels  \\\n",
       "0     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2     [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "3     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                 ...   \n",
       "1011  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "1012  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1013  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "1014  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1015  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                            head_labels  \n",
       "0     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "2     [[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...  \n",
       "3     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "4     [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "...                                                 ...  \n",
       "1011  [[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...  \n",
       "1012  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1013  [[0, 0, 0, 0, -1], [0, 0, 0, 0, 1], [0, 0, -1,...  \n",
       "1014  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1015  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "\n",
       "[1016 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------attempt of including all heads in one column--------------------\n",
    "labels = {'No Finding':0, 'Cardiomegaly':1,'Lung Opacity':2,'Edema':3,'Consolidation':4,\n",
    "          'Pneumonia':5,'Atelectasis':6,'Pneumothorax':7,'Pleural Effusion':8,'Fracture':9,'SupportDevices':10}\n",
    "def reorder(row,groups_index):\n",
    "#     print(row)\n",
    "    groups_values = []\n",
    "    for group in groups_index:\n",
    "        tmp =[]\n",
    "        for index in group:\n",
    "            if index==-1:\n",
    "                tmp.append(-1)\n",
    "            else:\n",
    "                tmp.append(row[index])\n",
    "        groups_values.append(tmp)\n",
    "#     print(groups_values)\n",
    "    return groups_values\n",
    "\n",
    "def group_heads(groups,df): # first attempt for each label one head\n",
    "    df['head_labels'] = df.apply(lambda row: reorder(row['labels'],groups), axis=1)\n",
    "    return df\n",
    "        \n",
    "def padding_heads(heads_index): #as heads does not have the same size, we pad them with -1\n",
    "    head_counts = [len(head) for head in heads_index]\n",
    "    lenmax = max(head_counts)\n",
    "#     print(lenmax)\n",
    "    padded_heads = []\n",
    "    for head in heads_index:\n",
    "        pad_n = lenmax - len(head)\n",
    "#         print(pad_n)\n",
    "        arr = np.pad(head, (0,pad_n), 'constant', constant_values=-1)\n",
    "        padded_heads.append(arr)\n",
    "    return padded_heads\n",
    "\n",
    "\n",
    "head_index1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "head_index2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "head_index3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [head_index1, head_index2, head_index3]\n",
    "\n",
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "\n",
    "padded_heads=padding_heads(heads_index)\n",
    "train_df = group_heads(padded_heads, train_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------attempt of one column per head-------------\n",
    "# def get_head(labels, head_index):\n",
    "#     head_values = []\n",
    "#     for index in head_index:\n",
    "#         head_values.append(labels[index])\n",
    "#     return head_values\n",
    "        \n",
    "# def split_data_to_heads(heads_index,df):\n",
    "#     i=1\n",
    "#     for head_index in heads_index:\n",
    "#         col_name = f\"head{i}\"\n",
    "#         df[col_name]= df.apply(lambda row: get_head(row['labels'],head_index), axis=1)\n",
    "#         i+=1\n",
    "#     return df\n",
    "    \n",
    "# train_df = split_data_to_heads(heads_index, train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head():\n",
    "    def __init__(self,hparams):\n",
    "        self.inputLayer = hparams['inputLayer']\n",
    "        self.run()\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "class HeadMultilabelCLS(Head):\n",
    "    def __init__(self,hparams):\n",
    "        self.labels = hparams['labels'] # batch labels, or all labels\n",
    "        self.device = hparams['device']\n",
    "        self.num_labels = hparams['num_labels'] # number of labels\n",
    "        self.taskspecificLayer = nn.Linear(hparams['input_size'], hparams['num_labels']).to(self.device) #classifier\n",
    "        self.loss = None\n",
    "        self.logits = None\n",
    "        self.pred_labels = None\n",
    "        \n",
    "        super().__init__(hparams)\n",
    "        \n",
    "    def run(self):\n",
    "        self.logits = self.taskspecificLayer(self.inputLayer)\n",
    "        self.pred_label = torch.sigmoid(self.logits)\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        self.loss = loss_func(self.logits.view(-1,self.num_labels),\n",
    "                         self.labels.type_as(self.logits).view(-1,self.num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        \n",
    "        return self.loss  \n",
    "    \n",
    "    \n",
    "# class HeadMulticlassCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         logits = self.taskspecificLayer(self.inputLayer)\n",
    "\n",
    "#         loss_fct = CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadBinaryCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "# class HeadAbstractiveSumm(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "# class HeadRegression(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "        \n",
    "#     def run(self):\n",
    "#         loss_fct = MSELoss()\n",
    "#         loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadSTSclinical(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "#         self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "#         self.head_count = config.head_count # ex:[2,3,4]\n",
    "#         self.head_index = config.head_index # ex: [[2,0],[1,3,4],[5,6,7,8,]]\n",
    "#         self.classifierheads = [nn.Linear(config.hidden_size, count) for count in self.head_count]\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "# [docs]\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "#     @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "#         #loop through the heads\n",
    "#         logit_heads = []\n",
    "#         for classifier in self.classifierheads:\n",
    "#             logits = classifier(pooled_output)\n",
    "#             logit_heads.append(logits)\n",
    "\n",
    "#         #TODO: work here on this for loop\n",
    "#         loss=0\n",
    "#         for logit,head_labels in zip(logit_heads):\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss += loss_func(logits.view(-1,head_num_labels),b_labels.type_as(logits).view(-1,head_num_labels)) #convert labels to float for calculation\n",
    "\n",
    "#         return loss  \n",
    "#-------------example------------------\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# import torch\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertCLS.from_pretrained('bert-base-uncased')\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# # labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(**inputs)\n",
    "# # loss, logits = outputs[:2]\n",
    "# type(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. fine tunning and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataLoader(input, labels, batch_size):\n",
    "    data = TensorDataset(input.input_ids, input.attention_mask, labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def calculate_f1_acc(pred_labels, true_labels, threshold = 0.50):\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('Validation F1: ', val_f1_accuracy)\n",
    "    print('Validation Accuracy: ', val_flat_accuracy)\n",
    "\n",
    "def calculate_f1_acc_test(pred_labels, true_labels, test_label_cols, threshold = 0.50):\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100 \n",
    "    print(\"-----------test-----------\")\n",
    "    print(\"Threshold: 0.5\")\n",
    "    print('Test F1 Score: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "    print('Test Accuracy: ', accuracy_score(true_bools, pred_bools))\n",
    "    print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) ,'\\n')\n",
    "    clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
    "    # pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "    print(clf_report)\n",
    "\n",
    "    # Calculate Accuracy - maximize F1 accuracy by tuning threshold values. First with 'macro_thresholds' on the order of e^-1 then with 'micro_thresholds' on the order of e^-2\n",
    "    print(\"-----Optimizing threshold value for micro F1 score-----\")\n",
    "\n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Score: ', f1_results[best_f1_idx])\n",
    "    print('Test Accuracy: ', flat_acc_results[best_f1_idx])\n",
    "    print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) , '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "    clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "    # pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "    print(clf_report_optimized)\n",
    "\n",
    "def multihead_cls(data_path, split_path, PreTrainedModel, epochs, batch_size, \n",
    "                  max_length ,ModelTokenizer, tokenizer_name, model_name, \n",
    "                  use_data_loader, heads_index, col_names):\n",
    "    #--------prepare the dataset-----------\n",
    "    train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "    num_labels = len(label_cols)\n",
    "    \n",
    "    padded_heads=padding_heads(heads_index)\n",
    "    train_df = group_heads(padded_heads, train_df)\n",
    "    test_df = group_heads(padded_heads, test_df)\n",
    "    val_df = group_heads(padded_heads, val_df)\n",
    "    \n",
    "    # ----------tokenize---------------\n",
    "    tokenizer = ModelTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    reports_train = train_df.text.to_list()\n",
    "    reports_test = test_df.text.to_list()\n",
    "    reports_val   = val_df.text.to_list()\n",
    "\n",
    "    train = tokenizer(reports_train, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    test = tokenizer(reports_test, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val = tokenizer(reports_val, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    #-----------dataloaders--------------\n",
    "    #prepare labels for dataloader\n",
    "    head_count = [len(group) for group in heads_index]\n",
    "    nheads = len(head_count)\n",
    "    \n",
    "    train_labels = torch.from_numpy(np.array(train_df.head_labels.to_list()))\n",
    "    test_labels = torch.from_numpy(np.array(test_df.head_labels.to_list()))\n",
    "    val_labels = torch.from_numpy(np.array(val_df.head_labels.to_list()))\n",
    "\n",
    "    \n",
    "    if use_data_loader: # if the dataset is huge in size\n",
    "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "        # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "        train_dataloader      = create_dataLoader(train, train_labels, batch_size)\n",
    "        validation_dataloader = create_dataLoader(val, val_labels, batch_size)\n",
    "        test_dataloader       = create_dataLoader(test, test_labels, batch_size)\n",
    "\n",
    "    else: #TODO: if the dataset is small in size\n",
    "        pass\n",
    "    \n",
    "    #-----------load model----------------\n",
    "    # Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
    "    model = PreTrainedModel.from_pretrained(model_name)\n",
    "    model.cuda()\n",
    "    optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n",
    "    \n",
    "    #---------FineTune model-----------\n",
    "#     device = \"cpu\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        #-------Training-------\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0 #running loss\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "#             print(b_labels)\n",
    "#             print(b_labels.size())\n",
    "#             print(b_labels[:,0,:])\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for multiclass classification\n",
    "            # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            # loss = outputs[0]\n",
    "            # logits = outputs[1]\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "#             outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "#             logits = outputs[0]\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             # loss_func = BCELoss() \n",
    "#             # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             train_loss_set.append(loss.item()) \n",
    "            \n",
    "            #Forward pass for multihead\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "#             print(outputs[0].size())\n",
    "            head_losses = []\n",
    "            for i in range(0,nheads):\n",
    "                \n",
    "                #remove -1 paddings:\n",
    "                labels = b_labels[:,i,:]\n",
    "                labels = labels[:,0:head_count[i]]\n",
    "                \n",
    "                hparams={\n",
    "                    'labels' : labels,\n",
    "                    'num_labels' : len(heads_index[i]),\n",
    "                    'input_size' : outputs[0].size()[0],\n",
    "                    'inputLayer' : outputs,\n",
    "                    'device'     : device,\n",
    "                }\n",
    "                head_losses.append(HeadMultilabelCLS(hparams))\n",
    "            \n",
    "            loss = 0\n",
    "            for head in head_losses:\n",
    "                loss += head.loss \n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # ---------Validation--------\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Variables to gather full output\n",
    "        true_labels_each_head,pred_labels_each_head = [],[]\n",
    "        true_labels_all_head,pred_labels_all_head = [],[]\n",
    "        \n",
    "        # Predict\n",
    "        for i, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "\n",
    "                pred_label_heads = []\n",
    "                true_label_heads = []\n",
    "                for i in range(0,nheads):\n",
    "                    #remove -1 paddings:\n",
    "                    labels = b_labels[:,i,:]\n",
    "                    labels = labels[:,0:head_count[i]]\n",
    "\n",
    "                    hparams={\n",
    "                        'labels' : labels,\n",
    "                        'num_labels' : len(heads_index[i]),\n",
    "                        'input_size' : outputs[0].size()[0],\n",
    "                        'inputLayer' : outputs,\n",
    "                        'device'     : device,\n",
    "                    }\n",
    "                    pred_label_heads.append(HeadMultilabelCLS(hparams).pred_label)\n",
    "                    true_label_heads.append(labels)\n",
    "                \n",
    "                #store batch labels \n",
    "                pred_label_b = np.array(pred_label_heads)\n",
    "                true_labels_b = np.array(true_label_heads)\n",
    "\n",
    "                #store each head label seperatly\n",
    "                true_labels_each_head.append(true_labels_b)\n",
    "                pred_labels_each_head.append(pred_label_b)\n",
    "\n",
    "                #store all head labels together\n",
    "                true_labels_all_head.append(\n",
    "                    torch.cat([true_head_label for true_head_label in true_labels_b],1)\n",
    "                    .to('cpu').numpy())\n",
    "                pred_labels_all_head.append(\n",
    "                    torch.cat([pred_head_label for pred_head_label in pred_label_b],1)\n",
    "                    .to('cpu').numpy())\n",
    "        \n",
    "        print(\"Results of all heads:\")\n",
    "        true_labels_all_head = np.concatenate([item for item in true_labels_all_head])\n",
    "        pred_labels_all_head = np.concatenate([item for item in pred_labels_all_head])\n",
    "#         print(\"true_labels_all_head\", true_labels_all_head.shape)\n",
    "#         print(\"pred_labels_all_head\", pred_labels_all_head.shape)\n",
    "        calculate_f1_acc(pred_labels_all_head, true_labels_all_head)\n",
    "        \n",
    "        true_labels_each_head = np.array(true_labels_each_head)\n",
    "        pred_labels_each_head = np.array(pred_labels_each_head)\n",
    "#         print(\"true_labels_each_head\", true_labels_each_head.shape)\n",
    "#         print(\"---true_labels_each_head\", true_labels_each_head)\n",
    "#         print(\"pred_labels_each_head\", pred_labels_each_head.shape)\n",
    "        \n",
    "        print(\"Results of each head:\")\n",
    "        for i in range(0,nheads):\n",
    "            print(f\"Head_{i}\")\n",
    "            i_head_true_labels = true_labels_each_head[:,i]\n",
    "            i_head_true_labels = torch.cat([item for item in i_head_true_labels],0).to('cpu').numpy()\n",
    "            \n",
    "            i_head_pred_labels = pred_labels_each_head[:,i]\n",
    "            i_head_pred_labels = torch.cat([item for item in i_head_pred_labels],0).to('cpu').numpy()\n",
    "            calculate_f1_acc(i_head_pred_labels, i_head_true_labels)\n",
    "\n",
    "    # ---------test--------\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    #track variables\n",
    "    true_labels_each_head,pred_labels_each_head = [],[]\n",
    "    true_labels_all_head,pred_labels_all_head = [],[]\n",
    "#     Predict\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "\n",
    "            pred_label_heads = []\n",
    "            true_label_heads = []\n",
    "            for i in range(0,nheads):\n",
    "                #remove -1 paddings:\n",
    "                labels = b_labels[:,i,:]\n",
    "                labels = labels[:,0:head_count[i]]\n",
    "\n",
    "                hparams={\n",
    "                    'labels' : labels,\n",
    "                    'num_labels' : len(heads_index[i]),\n",
    "                    'input_size' : outputs[0].size()[0],\n",
    "                    'inputLayer' : outputs,\n",
    "                    'device'     : device,\n",
    "                }\n",
    "                pred_label_heads.append(HeadMultilabelCLS(hparams).pred_label)\n",
    "                true_label_heads.append(labels)\n",
    "\n",
    "            #store batch labels \n",
    "            pred_label_b = np.array(pred_label_heads)\n",
    "            true_labels_b = np.array(true_label_heads)\n",
    "\n",
    "            #store each head label seperatly\n",
    "            true_labels_each_head.append(true_labels_b)\n",
    "            pred_labels_each_head.append(pred_label_b)\n",
    "\n",
    "            #store all head labels together\n",
    "            true_labels_all_head.append(\n",
    "                torch.cat([true_head_label for true_head_label in true_labels_b],1)\n",
    "                .to('cpu').numpy())\n",
    "            pred_labels_all_head.append(\n",
    "                torch.cat([pred_head_label for pred_head_label in pred_label_b],1)\n",
    "                .to('cpu').numpy())\n",
    "\n",
    "    print(\"########## Results of all heads:\")\n",
    "    true_labels_all_head = np.concatenate([item for item in true_labels_all_head])\n",
    "    pred_labels_all_head = np.concatenate([item for item in pred_labels_all_head])\n",
    "    calculate_f1_acc_test(pred_labels_all_head, true_labels_all_head, col_names)\n",
    "\n",
    "    true_labels_each_head = np.array(true_labels_each_head)\n",
    "    pred_labels_each_head = np.array(pred_labels_each_head)\n",
    "\n",
    "    print(\"########### Results of each head:\")\n",
    "    for i in range(0,nheads):\n",
    "        print(f\"Head_{i}\")\n",
    "        i_head_true_labels = true_labels_each_head[:,i]\n",
    "        i_head_true_labels = torch.cat([item for item in i_head_true_labels],0).to('cpu').numpy()\n",
    "\n",
    "        i_head_pred_labels = pred_labels_each_head[:,i]\n",
    "        i_head_pred_labels = torch.cat([item for item in i_head_pred_labels],0).to('cpu').numpy()\n",
    "        calculate_f1_acc(i_head_pred_labels, i_head_true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertCLS: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertCLS from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertCLS from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.11224571056664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:14<00:28, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of all heads:\n",
      "Validation F1:  18.497576736672052\n",
      "Validation Accuracy:  0.0\n",
      "Results of each head:\n",
      "Head_0\n",
      "Validation F1:  31.500465983224608\n",
      "Validation Accuracy:  9.392265193370166\n",
      "Head_1\n",
      "Validation F1:  4.197271773347324\n",
      "Validation Accuracy:  0.8287292817679558\n",
      "Head_2\n",
      "Validation F1:  17.777777777777775\n",
      "Validation Accuracy:  26.243093922651934\n",
      "Train loss: 2.0907000582665205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:28<00:14, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of all heads:\n",
      "Validation F1:  18.98527004909984\n",
      "Validation Accuracy:  1.1049723756906076\n",
      "Results of each head:\n",
      "Head_0\n",
      "Validation F1:  35.37549407114624\n",
      "Validation Accuracy:  8.83977900552486\n",
      "Head_1\n",
      "Validation F1:  4.75206611570248\n",
      "Validation Accuracy:  9.116022099447514\n",
      "Head_2\n",
      "Validation F1:  12.931034482758621\n",
      "Validation Accuracy:  19.88950276243094\n",
      "Train loss: 2.1080818325281143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:42<00:00, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of all heads:\n",
      "Validation F1:  20.60461416070008\n",
      "Validation Accuracy:  0.0\n",
      "Results of each head:\n",
      "Head_0\n",
      "Validation F1:  35.746201966041106\n",
      "Validation Accuracy:  9.668508287292818\n",
      "Head_1\n",
      "Validation F1:  5.4\n",
      "Validation Accuracy:  0.2762430939226519\n",
      "Head_2\n",
      "Validation F1:  16.20253164556962\n",
      "Validation Accuracy:  32.04419889502763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Results of all heads:\n",
      "-----------test-----------\n",
      "Threshold: 0.5\n",
      "Test F1 Score:  0.1995268138801262\n",
      "Test Accuracy:  0.0\n",
      "LRAP:  0.30350432470968197 \n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Finding       0.33      0.55      0.41       130\n",
      "    Cardiomegaly       0.22      0.55      0.31        83\n",
      "    Lung Opacity       0.26      0.42      0.32        95\n",
      "           Edema       0.21      0.44      0.28        75\n",
      "   Consolidation       0.05      0.44      0.09        16\n",
      "       Pneumonia       0.02      0.80      0.04         5\n",
      "     Atelectasis       0.04      0.73      0.07        11\n",
      "    Pneumothorax       0.01      0.25      0.01         4\n",
      "Pleural Effusion       0.05      0.38      0.10        21\n",
      "        Fracture       0.11      0.66      0.19        35\n",
      "  SupportDevices       0.06      0.40      0.11        30\n",
      "\n",
      "       micro avg       0.12      0.50      0.20       505\n",
      "       macro avg       0.12      0.51      0.18       505\n",
      "    weighted avg       0.22      0.50      0.29       505\n",
      "     samples avg       0.13      0.51      0.19       505\n",
      "\n",
      "-----Optimizing threshold value for micro F1 score-----\n",
      "Best Threshold:  0.1\n",
      "Test F1 Score:  0.22399645154136172\n",
      "Test Accuracy:  0.0\n",
      "LRAP:  0.30350432470968197 \n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Finding       0.36      1.00      0.53       130\n",
      "    Cardiomegaly       0.23      1.00      0.37        83\n",
      "    Lung Opacity       0.26      1.00      0.41        95\n",
      "           Edema       0.21      1.00      0.34        75\n",
      "   Consolidation       0.04      1.00      0.08        16\n",
      "       Pneumonia       0.01      1.00      0.03         5\n",
      "     Atelectasis       0.03      1.00      0.06        11\n",
      "    Pneumothorax       0.01      1.00      0.02         4\n",
      "Pleural Effusion       0.06      1.00      0.11        21\n",
      "        Fracture       0.10      1.00      0.18        35\n",
      "  SupportDevices       0.08      1.00      0.15        30\n",
      "\n",
      "       micro avg       0.13      1.00      0.22       505\n",
      "       macro avg       0.13      1.00      0.21       505\n",
      "    weighted avg       0.23      1.00      0.36       505\n",
      "     samples avg       0.13      1.00      0.22       505\n",
      "\n",
      "########### Results of each head:\n",
      "Head_0\n",
      "Validation F1:  33.77777777777778\n",
      "Validation Accuracy:  7.6923076923076925\n",
      "Head_1\n",
      "Validation F1:  5.894736842105263\n",
      "Validation Accuracy:  1.9230769230769231\n",
      "Head_2\n",
      "Validation F1:  15.184381778741868\n",
      "Validation Accuracy:  16.483516483516482\n"
     ]
    }
   ],
   "source": [
    "#------------dataset details------------\n",
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "use_data_loader = True\n",
    "#--------------training details-----------\n",
    "epochs = 3 # Number of training epochs (authors recommend between 2 and 4)\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "#---------------saving results details----------\n",
    "experiment_name = \"bert\"\n",
    "#----------------bert---------------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "\n",
    "# # model_name = \"bert-base-cased\"\n",
    "# # tokenizer_name = \"bert-base-cased\"\n",
    "\n",
    "ModelTokenizer = BertTokenizer\n",
    "PreTrainedModel = BertCLS #BertForMultiheadClassification\n",
    "\n",
    "#----------------BioBERT-v1.0----------\n",
    "# model_name = \"monologg/biobert_v1.0_pubmed_pmc\" # from hugginface model list\n",
    "# model_name = \"model_wieghts/biobert_v1.0_pubmed_pmc\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#----------------BioBERT-v1.1----------\n",
    "# model_name = \"model_wieghts/biobert_v1.1_pubmed\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#------------roberta-------------\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# model_name = 'roberta-base'\n",
    "# tokenizer_name = 'roberta-base'\n",
    "# ModelTokenizer = RobertaTokenizer\n",
    "# PreTrainedModel = RobertaForSequenceClassification\n",
    "\n",
    "#------------albert-------------\n",
    "# from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "# model_name = 'albert-base-v1'\n",
    "# tokenizer_name = 'albert-base-v1'\n",
    "# ModelTokenizer = AlbertTokenizer\n",
    "# PreTrainedModel = AlbertForSequenceClassification\n",
    "\n",
    "#medical sense\n",
    "group1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "group2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "group3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [group1, group2, group3]\n",
    "col_names = ['No Finding', 'Cardiomegaly', 'Lung Opacity', 'Edema', 'Consolidation', 'Pneumonia', \n",
    "             'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Fracture', 'SupportDevices']\n",
    "multihead_cls(data_path,split_path, PreTrainedModel, epochs, batch_size, max_length ,\n",
    "               ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index, col_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_grouping(observations):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
