{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, label_ranking_average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task heads\n",
    "In this experiment, we combine different classes and group them in different heads. We then experiment to check if we are getting any better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path,split_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    cols = df.columns\n",
    "    label_cols = list(cols[6:])\n",
    "    \n",
    "\n",
    "    train_df = pd.read_csv(f\"{split_path}/train.csv\")\n",
    "    # convert str to list\n",
    "    train_df['labels'] = train_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    test_df = pd.read_csv(f\"{split_path}/test.csv\")\n",
    "    test_df['labels'] = test_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    val_df = pd.read_csv(f\"{split_path}/val.csv\")\n",
    "    val_df['labels'] = val_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    return train_df, test_df, val_df, label_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Grouping the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "0\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>head_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heart size is normal and lungs are clear. No p...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiomediastinal silhouette is normal. Pulmon...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardiomegaly. Left lung clear. Large right eff...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal cardiac size and contour unremarkable m...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The cardiac and mediastinal contours are withi...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The cardiomediastinal silhouette and pulmonary...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Atrial septal occluder artifact. Rotated front...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 1, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>opacities in the left base may be compatible ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No focal consolidation. No pneumothorax. No pl...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The trachea is midline. Cardiomediastinal silh...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The cardiomediastinal silhouette is stable in ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Anterior segment right lower lobe pneumonia. N...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 1, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The patchy right lower lobe and left lower lob...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The lungs are clear. The cardiomediastinal sil...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The heart is large, and the pulmonary  are eng...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Spinal stimulator in . Lungs are clear without...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The heart size is stable. The aorta is ectatic...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>There is a 1.5 cm nodular opacity projecting o...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>There is stable elevation of the right hemidia...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 1, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Slight cardiomegaly. Lungs are clear. No rib a...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   Heart size is normal and lungs are clear. No p...   \n",
       "1   Cardiomediastinal silhouette is normal. Pulmon...   \n",
       "2   Cardiomegaly. Left lung clear. Large right eff...   \n",
       "3   Normal cardiac size and contour unremarkable m...   \n",
       "4   The cardiac and mediastinal contours are withi...   \n",
       "5   The cardiomediastinal silhouette and pulmonary...   \n",
       "6   Atrial septal occluder artifact. Rotated front...   \n",
       "7    opacities in the left base may be compatible ...   \n",
       "8   No focal consolidation. No pneumothorax. No pl...   \n",
       "9   The trachea is midline. Cardiomediastinal silh...   \n",
       "10  The cardiomediastinal silhouette is stable in ...   \n",
       "11  Anterior segment right lower lobe pneumonia. N...   \n",
       "12  The patchy right lower lobe and left lower lob...   \n",
       "13  The lungs are clear. The cardiomediastinal sil...   \n",
       "14  The heart is large, and the pulmonary  are eng...   \n",
       "15  Spinal stimulator in . Lungs are clear without...   \n",
       "16  The heart size is stable. The aorta is ectatic...   \n",
       "17  There is a 1.5 cm nodular opacity projecting o...   \n",
       "18  There is stable elevation of the right hemidia...   \n",
       "19  Slight cardiomegaly. Lungs are clear. No rib a...   \n",
       "\n",
       "                               labels  \\\n",
       "0   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "3   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6   [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "7   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "12  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "13  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "14  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "15  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "16  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "17  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "18  [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "19  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                          head_labels  \n",
       "0   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "2   [[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...  \n",
       "3   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "4   [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "5   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "6   [[0, 0, 1, 1, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "7   [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "8   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "9   [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "10  [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "11  [[0, 0, 0, 0, -1], [0, 0, 1, 0, 0], [0, 0, -1,...  \n",
       "12  [[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "13  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "14  [[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "15  [[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...  \n",
       "16  [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "17  [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "18  [[0, 0, 1, 1, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "19  [[0, 1, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {'No Finding':0, 'Cardiomegaly':1,'Lung Opacity':2,'Edema':3,'Consolidation':4,\n",
    "          'Pneumonia':5,'Atelectasis':6,'Pneumothorax':7,'Pleural Effusion':8,'Fracture':9,'SupportDevices':10}\n",
    "def reorder(row,groups_index):\n",
    "#     print(row)\n",
    "    groups_values = []\n",
    "    for group in groups_index:\n",
    "        tmp =[]\n",
    "        for index in group:\n",
    "            if index==-1:\n",
    "                tmp.append(-1)\n",
    "            else:\n",
    "                tmp.append(row[index])\n",
    "        groups_values.append(tmp)\n",
    "#     print(groups_values)\n",
    "    return groups_values\n",
    "\n",
    "\n",
    "def group_heads(groups,df): # first attempt for each label one head\n",
    "    df['head_labels'] = df.apply(lambda row: reorder(row['labels'],groups), axis=1)\n",
    "    return df\n",
    "        \n",
    "def padding_heads(heads_index): #as heads does not have the same size, we pad them with -1\n",
    "    head_counts = [len(head) for head in heads_index]\n",
    "    lenmax = max(head_counts)\n",
    "    print(lenmax)\n",
    "    padded_heads = []\n",
    "    for head in heads_index:\n",
    "        pad_n = lenmax - len(head)\n",
    "        print(pad_n)\n",
    "        arr = np.pad(head, (0,pad_n), 'constant', constant_values=-1)\n",
    "        padded_heads.append(arr)\n",
    "    return padded_heads\n",
    "\n",
    "group1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "group2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "group3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [group1, group2, group3]\n",
    "\n",
    "padded_heads=padding_heads(heads_index)\n",
    "\n",
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "\n",
    "train_df = group_heads(padded_heads, train_df)\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. fine tunning and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head():\n",
    "    def __init__(self,hparams):\n",
    "        self.inputLayer = hparams.inputLayer\n",
    "        self.taskspecificLayer = None\n",
    "        self.loss = None\n",
    "        self.run()\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "class HeadMultilabelCLS(Head):\n",
    "    def __init__(self,hparams):\n",
    "        super().__init__(hparams)\n",
    "        self.labels = hparams.labels # batch labels, or all labels\n",
    "        self.num_labels = hparams.num_labels # number of labels\n",
    "        self.taskspecificLayer = nn.Linear(hparams.hidden_size, hparams.num_labels) #classifier\n",
    "\n",
    "    def run(self):\n",
    "        logits = self.taskspecificLayer(self.inputLayer)\n",
    "\n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,self.num_labels),self.labels.type_as(logits).view(-1,self.num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "        return loss  \n",
    "    \n",
    "    \n",
    "# class HeadMulticlassCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         logits = self.taskspecificLayer(self.inputLayer)\n",
    "\n",
    "#         loss_fct = CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadBinaryCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "# class HeadAbstractiveSumm(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "# class HeadRegression(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "        \n",
    "#     def run(self):\n",
    "#         loss_fct = MSELoss()\n",
    "#         loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadSTSclinical(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooook\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "class Ape(object):\n",
    "    def __init__(self):\n",
    "        print ('ooook')\n",
    "        self.say('hi')\n",
    "    def say(self, s):\n",
    "        print (s)\n",
    "\n",
    "def main():\n",
    "    Ape()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultiheadClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiheadClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiheadClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "#         self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "#         self.head_count = config.head_count # ex:[2,3,4]\n",
    "#         self.head_index = config.head_index # ex: [[2,0],[1,3,4],[5,6,7,8,]]\n",
    "#         self.classifierheads = [nn.Linear(config.hidden_size, count) for count in self.head_count]\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "# [docs]\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "#     @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "#         #loop through the heads\n",
    "#         logit_heads = []\n",
    "#         for classifier in self.classifierheads:\n",
    "#             logits = classifier(pooled_output)\n",
    "#             logit_heads.append(logits)\n",
    "\n",
    "#         #TODO: work here on this for loop\n",
    "#         loss=0\n",
    "#         for logit,head_labels in zip(logit_heads):\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss += loss_func(logits.view(-1,head_num_labels),b_labels.type_as(logits).view(-1,head_num_labels)) #convert labels to float for calculation\n",
    "\n",
    "#         return loss  \n",
    "#-------------example------------------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMultiheadClassification.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs)\n",
    "# loss, logits = outputs[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
       "          2.2184e-02,  5.7299e-01,  6.2331e-02,  5.9586e-02, -9.9965e-01,\n",
       "          5.0146e-02,  4.4756e-01,  9.7612e-01,  3.3988e-02,  8.4494e-01,\n",
       "         -3.6905e-01,  9.8649e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
       "          4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
       "          6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
       "         -3.2187e-01,  8.6672e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
       "         -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
       "         -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1517e-02,\n",
       "         -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3119e-02,\n",
       "          3.6740e-01, -7.3230e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
       "          4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
       "         -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9813e-01,\n",
       "          1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
       "          6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
       "          8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
       "          3.9848e-01,  2.1360e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
       "         -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
       "          3.9138e-01, -2.1884e-02, -9.6332e-02,  3.8545e-01, -3.4136e-01,\n",
       "         -8.0362e-02, -3.2022e-02, -3.6328e-01, -7.8130e-02,  1.9192e-02,\n",
       "         -1.3429e-01, -1.6013e-02, -5.2640e-02, -2.8006e-01,  9.3611e-02,\n",
       "         -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
       "          2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
       "         -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
       "          3.7344e-02, -1.9320e-01,  9.1691e-01,  3.6012e-01,  1.4505e-01,\n",
       "          1.5398e-01, -1.0657e-02, -1.0000e+00, -3.1573e-01, -3.1037e-01,\n",
       "          1.6523e-01, -8.0330e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
       "          9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
       "          2.5317e-01, -2.0185e-01,  2.9533e-02, -2.3162e-01,  3.4632e-01,\n",
       "         -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1134e-02,\n",
       "         -9.6283e-02, -7.6358e-02, -6.5149e-02, -8.9277e-01, -2.3465e-01,\n",
       "          9.1176e-01,  7.0429e-02, -2.1429e-01,  3.8197e-01,  3.5892e-02,\n",
       "         -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
       "          2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
       "          1.8090e-01, -3.2497e-02, -3.0137e-01, -9.7103e-01, -1.3917e-01,\n",
       "          3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8812e-02,  1.3991e-02,\n",
       "         -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
       "          1.5224e-01, -1.8241e-01,  5.5584e-02, -7.7306e-01, -9.9000e-02,\n",
       "          4.7058e-01, -1.7022e-01, -7.7803e-01,  5.2834e-02, -3.7679e-01,\n",
       "         -4.1296e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
       "          5.0384e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
       "          2.8563e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
       "          3.1902e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
       "         -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0617e-03, -4.1152e-01,\n",
       "          3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
       "         -4.8133e-01,  9.3245e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
       "          7.7973e-01,  4.6696e-01, -3.4636e-01,  5.5238e-02,  9.0312e-01,\n",
       "         -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
       "         -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
       "         -2.6726e-01, -5.4679e-01,  9.3944e-04, -1.0502e-02, -4.6837e-01,\n",
       "          3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
       "          8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
       "         -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
       "         -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6622e-02,  8.5571e-01,\n",
       "         -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
       "          8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
       "          3.3430e-01,  2.1397e-01,  3.0630e-02,  9.3878e-01,  9.9986e-01,\n",
       "          9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
       "          9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
       "          2.3399e-02, -1.0000e+00, -6.1938e-02,  1.9563e-01, -9.0552e-01,\n",
       "         -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
       "          8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4073e-01,  9.6085e-01,\n",
       "          1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
       "         -5.9538e-01,  3.7464e-01, -2.1189e-01,  8.8024e-01,  1.9648e-02,\n",
       "         -3.8349e-01, -8.4779e-01,  1.4676e-02, -2.8375e-02, -4.4313e-01,\n",
       "         -9.4966e-01, -6.5704e-02, -7.2327e-02,  6.5967e-01, -1.1504e-01,\n",
       "          2.1876e-01, -5.5254e-01,  9.2219e-02, -5.0583e-01, -5.2826e-02,\n",
       "          5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
       "         -3.1652e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
       "          1.0000e+00,  1.1755e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
       "         -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
       "          2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
       "          7.8269e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
       "         -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
       "         -4.9026e-01, -1.2517e-01, -3.9076e-02,  8.8128e-02, -8.2481e-01,\n",
       "          3.8301e-01,  1.8045e-01,  5.4796e-02,  8.0041e-01, -3.3501e-01,\n",
       "         -3.9115e-01,  1.4233e-01, -9.0141e-02,  3.4585e-01,  4.4044e-01,\n",
       "          3.1044e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
       "         -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0752e-02,  8.3374e-02,\n",
       "         -3.1301e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
       "         -1.4267e-01, -7.4000e-01,  2.2856e-01, -7.9442e-01, -9.8812e-01,\n",
       "          4.3592e-01,  7.7229e-02, -3.8084e-02,  9.9490e-01,  3.2616e-01,\n",
       "          6.7989e-02,  8.2888e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
       "          3.7665e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
       "         -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
       "          2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
       "          7.3959e-02,  3.1273e-02,  1.0000e+00, -7.5631e-01,  3.5396e-01,\n",
       "          5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
       "          3.5377e-02, -4.6060e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
       "          3.4662e-02,  4.5219e-03, -8.8296e-01, -9.8300e-01,  1.6468e-01,\n",
       "          3.3595e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
       "          1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
       "          2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9311e-01,  8.2643e-01,\n",
       "          3.1897e-02,  1.8036e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
       "         -4.1523e-01, -2.0335e-01,  5.0975e-03,  1.0000e+00, -1.3769e-01,\n",
       "          4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
       "          5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1692e-01,  3.3706e-01,\n",
       "         -9.4995e-02,  3.3125e-01, -1.1600e-01,  5.5663e-02,  6.9017e-01,\n",
       "          5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
       "          4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
       "          4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5118e-02, -9.8683e-01,\n",
       "         -3.4659e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
       "          4.0561e-01,  5.1398e-01, -7.1926e-03,  3.7469e-01, -4.9920e-02,\n",
       "         -1.8379e-01,  1.0699e-01,  6.4271e-02,  9.4363e-01, -4.5982e-01,\n",
       "         -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
       "         -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
       "         -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7384e-02,  9.4459e-01,\n",
       "          8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5494e-02,  2.9531e-01,\n",
       "         -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
       "          9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
       "          1.1140e-01, -7.1071e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
       "          7.1903e-03,  9.3813e-02,  1.2718e-01, -4.2175e-01,  6.2383e-01,\n",
       "         -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
       "          5.2774e-01, -6.4998e-02,  3.8218e-02, -1.3764e-01,  1.3114e-01,\n",
       "         -8.2896e-01, -6.2801e-02, -1.3077e-01, -9.9745e-01,  3.8189e-01,\n",
       "         -1.0000e+00, -4.9528e-02, -3.3011e-01, -9.7048e-03,  7.4032e-01,\n",
       "          4.5588e-01, -4.3038e-02, -5.9485e-01,  3.5139e-02,  8.4290e-01,\n",
       "          7.0024e-01,  4.9502e-03,  1.5221e-01, -4.8182e-01,  3.4912e-02,\n",
       "          6.8681e-02,  5.9797e-02,  9.4147e-02,  5.7532e-01,  3.5063e-02,\n",
       "          1.0000e+00, -4.4784e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
       "         -4.8241e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
       "         -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
       "         -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2237e-01,  2.1811e-01,\n",
       "         -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0503e-02,  9.8319e-01,\n",
       "          3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1441e-02,  9.3666e-01,\n",
       "          7.3988e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
       "         -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
       "          2.1140e-01, -3.4107e-02,  8.0942e-01,  4.9841e-03,  8.9624e-01,\n",
       "          6.7186e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
       "         -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
       "          1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
       "         -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
       "          4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
       "         -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0548e-01,\n",
       "          2.5306e-01, -4.0736e-01, -3.4586e-02,  4.1000e-01, -8.3874e-01,\n",
       "         -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
       "         -5.9613e-01, -1.9438e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
       "          5.9324e-01, -1.8357e-02, -6.8000e-03,  2.1554e-01,  8.2484e-01,\n",
       "          8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
       "          2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3628e-03, -3.2062e-02,\n",
       "         -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2645e-01,  6.3986e-01,\n",
       "         -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
       "          6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3170e-01,  5.3485e-02,\n",
       "          8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
       "         -5.3294e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1458e-01,\n",
       "         -6.7959e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
       "         -6.4847e-02, -4.3871e-01,  6.3752e-01, -2.1017e-01, -2.9291e-01,\n",
       "         -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4508e-02,\n",
       "         -2.9022e-01, -2.1880e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
       "         -9.9998e-01,  1.4066e-01,  9.1606e-02,  1.1457e-01, -2.1965e-01,\n",
       "          3.0746e-01, -5.7719e-02, -8.7692e-01, -9.3891e-02,  2.2809e-01,\n",
       "          3.8767e-02, -3.2828e-01, -3.1138e-01,  4.1117e-01,  4.6004e-01,\n",
       "          5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
       "         -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataLoader(input, labels, batch_size):\n",
    "    data = TensorDataset(input.input_ids, input.attention_mask, labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def multihead_cls(data_path, split_path, PreTrainedModel, epochs, batch_size, max_length ,ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index):\n",
    "    #prepare the dataset\n",
    "    train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "    \n",
    "#     train_df = group_heads(heads_index, train_df)\n",
    "#     val_df = group_heads(heads_index, val_df)\n",
    "#     test_df = group_heads(heads_index, test_df)\n",
    "    \n",
    "#     num_labels = len(label_cols)\n",
    "    head_count = [len(group) for group in heads_index]\n",
    "    # ----------tokenize---------------\n",
    "    tokenizer = ModelTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    reports_train = train_df.text.to_list()\n",
    "    reports_test = test_df.text.to_list()\n",
    "    reports_val   = val_df.text.to_list()\n",
    "\n",
    "    train = tokenizer(reports_train, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    test = tokenizer(reports_test, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val = tokenizer(reports_val, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "#     print(\"***\")\n",
    "#     print(type(train_df.head_labels.to_list()))\n",
    "#     print(np.array(train_df.head_labels.to_list(), dtype=\"int32\").dtype)\n",
    "    train_labels = torch.from_numpy(np.array(train_df.labels.to_list()))\n",
    "#     train_labels = train_df.head_labels\n",
    "    test_labels = torch.from_numpy(np.array(test_df.labels.to_list()))\n",
    "#     test_labels = test_df.head_labels\n",
    "    val_labels = torch.from_numpy(np.array(val_df.labels.to_list()))\n",
    "#     val_labels = val_df.head_labels\n",
    "\n",
    "    #-----------dataloaders--------------\n",
    "    if use_data_loader: # if the dataset is huge in size\n",
    "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "        # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "        train_dataloader = create_dataLoader(train, train_labels, batch_size)\n",
    "        validation_dataloader   = create_dataLoader(val, val_labels, batch_size)\n",
    "        test_dataloader  = create_dataLoader(test, test_labels, batch_size)\n",
    "\n",
    "    else: #TODO: if the dataset is small in size\n",
    "        pass\n",
    "\n",
    "    # Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
    "    model = BertForMultiheadClassification(model_name, head_counts = head_counts, head_index = head_index)\n",
    "    model.cuda()\n",
    "    optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n",
    "    \n",
    "    #---------FineTune model-----------\n",
    "    device = \"cpu\"\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        #-------Training-------\n",
    "\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0 #running loss\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for multiclass classification\n",
    "            # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            # loss = outputs[0]\n",
    "            # logits = outputs[1]\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "#             outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "#             logits = outputs[0]\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             # loss_func = BCELoss() \n",
    "#             # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             train_loss_set.append(loss.item()) \n",
    "            \n",
    "            #Forward pass for multioutput classification\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            train_loss_set.append(loss.item()) \n",
    "            \n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # ---------Validation--------\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Variables to gather full output\n",
    "        logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "        # Predict\n",
    "        for i, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                b_logit_pred = outs[0]\n",
    "                pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "                b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "                pred_label = pred_label.to('cpu').numpy()\n",
    "                b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tokenized_texts.append(b_input_ids)\n",
    "            logit_preds.append(b_logit_pred)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "\n",
    "        # Flatten outputs\n",
    "        pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "        true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        threshold = 0.50\n",
    "        pred_bools = [pl>threshold for pl in pred_labels]\n",
    "        true_bools = [tl==1 for tl in true_labels]\n",
    "        val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "        val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "        print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "        print('Validation Accuracy: ', val_flat_accuracy)\n",
    "\n",
    "    # ---------test--------\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    #track variables\n",
    "    logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "    # Predict\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            b_logit_pred = outs[0]\n",
    "            pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "            b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tokenized_texts.append(b_input_ids)\n",
    "        logit_preds.append(b_logit_pred)\n",
    "        true_labels.append(b_labels)\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "    pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    # Converting flattened binary values to boolean values\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "\n",
    "    #We need to threshold our sigmoid function outputs which range from [0, 1]. Below I use 0.50 as a threshold.\n",
    "    test_label_cols = label_cols\n",
    "    pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
    "\n",
    "    # Print and save classification report\n",
    "    print(\"-----------test-----------\")\n",
    "    print(\"Threshold: 0.5\")\n",
    "    print('Test F1 Score: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "    print('Test Accuracy: ', accuracy_score(true_bools, pred_bools))\n",
    "    print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) ,'\\n')\n",
    "    clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
    "    # pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "    print(clf_report)\n",
    "\n",
    "    # Calculate Accuracy - maximize F1 accuracy by tuning threshold values. First with 'macro_thresholds' on the order of e^-1 then with 'micro_thresholds' on the order of e^-2\n",
    "    print(\"-----Optimizing threshold value for micro F1 score-----\")\n",
    "\n",
    "    macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in macro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "    micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "    f1_results, flat_acc_results = [], []\n",
    "    for th in micro_thresholds:\n",
    "        pred_bools = [pl>th for pl in pred_labels]\n",
    "        test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "        test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "        f1_results.append(test_f1_accuracy)\n",
    "        flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "    print('Test F1 Score: ', f1_results[best_f1_idx])\n",
    "    print('Test Accuracy: ', flat_acc_results[best_f1_idx])\n",
    "    print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) , '\\n')\n",
    "\n",
    "    best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "    clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "    # pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "    print(clf_report_optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'head_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e4288ff219a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m multihead_cls(data_path,split_path, PreTrainedModel, epochs, batch_size, max_length ,\n\u001b[0;32m---> 54\u001b[0;31m                ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a9de5e7e48f6>\u001b[0m in \u001b[0;36mmultihead_cls\u001b[0;34m(data_path, split_path, PreTrainedModel, epochs, batch_size, max_length, ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     print(type(train_df.head_labels.to_list()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     print(np.array(train_df.head_labels.to_list(), dtype=\"int32\").dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#     train_labels = train_df.head_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'head_labels'"
     ]
    }
   ],
   "source": [
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "use_data_loader = True\n",
    "\n",
    "epochs = 3 # Number of training epochs (authors recommend between 2 and 4)\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "#----------------bert---------------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "\n",
    "# # model_name = \"bert-base-cased\"\n",
    "# # tokenizer_name = \"bert-base-cased\"\n",
    "\n",
    "ModelTokenizer = BertTokenizer\n",
    "PreTrainedModel = BertForMultiheadClassification\n",
    "\n",
    "#----------------BioBERT-v1.0----------\n",
    "# model_name = \"monologg/biobert_v1.0_pubmed_pmc\" # from hugginface model list\n",
    "# model_name = \"model_wieghts/biobert_v1.0_pubmed_pmc\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#----------------BioBERT-v1.1----------\n",
    "# model_name = \"model_wieghts/biobert_v1.1_pubmed\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#------------roberta-------------\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# model_name = 'roberta-base'\n",
    "# tokenizer_name = 'roberta-base'\n",
    "# ModelTokenizer = RobertaTokenizer\n",
    "# PreTrainedModel = RobertaForSequenceClassification\n",
    "\n",
    "#------------albert-------------\n",
    "# from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "# model_name = 'albert-base-v1'\n",
    "# tokenizer_name = 'albert-base-v1'\n",
    "# ModelTokenizer = AlbertTokenizer\n",
    "# PreTrainedModel = AlbertForSequenceClassification\n",
    "\n",
    "\n",
    "group1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "group2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "group3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [group1, group2, group3]\n",
    "\n",
    "\n",
    "multihead_cls(data_path,split_path, PreTrainedModel, epochs, batch_size, max_length ,\n",
    "               ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "\n",
    "c = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5, -1, -1, -1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "np.pad(a, (0,3), 'constant', constant_values=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
