a{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, label_ranking_average_precision_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task heads\n",
    "In this experiment, we combine different classes and group them in different heads. We then experiment to check if we are getting any better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path,split_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    cols = df.columns\n",
    "    label_cols = list(cols[6:])\n",
    "    \n",
    "\n",
    "    train_df = pd.read_csv(f\"{split_path}/train.csv\")\n",
    "    # convert str to list\n",
    "    train_df['labels'] = train_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    test_df = pd.read_csv(f\"{split_path}/test.csv\")\n",
    "    test_df['labels'] = test_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    val_df = pd.read_csv(f\"{split_path}/val.csv\")\n",
    "    val_df['labels'] = val_df.apply(lambda row: ast.literal_eval(row['labels']), axis=1)\n",
    "\n",
    "    return train_df, test_df, val_df, label_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create a column for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>head_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heart size is normal and lungs are clear. No p...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiomediastinal silhouette is normal. Pulmon...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cardiomegaly. Left lung clear. Large right eff...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal cardiac size and contour unremarkable m...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The cardiac and mediastinal contours are withi...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>Frontal and lateral views of the chest with ov...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>Lungs are clear. Heart size normal. No pneumot...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Cardiomediastinal contour and pulmonary vascul...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[0, 0, 0, 0, -1], [0, 0, 0, 0, 1], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>The lungs are clear bilaterally. Specifically,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>Heart size is normal. Lungs are clear. No pneu...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Heart size is normal and lungs are clear. No p...   \n",
       "1     Cardiomediastinal silhouette is normal. Pulmon...   \n",
       "2     Cardiomegaly. Left lung clear. Large right eff...   \n",
       "3     Normal cardiac size and contour unremarkable m...   \n",
       "4     The cardiac and mediastinal contours are withi...   \n",
       "...                                                 ...   \n",
       "1011  Frontal and lateral views of the chest with ov...   \n",
       "1012  Lungs are clear. Heart size normal. No pneumot...   \n",
       "1013  Cardiomediastinal contour and pulmonary vascul...   \n",
       "1014  The lungs are clear bilaterally. Specifically,...   \n",
       "1015  Heart size is normal. Lungs are clear. No pneu...   \n",
       "\n",
       "                                 labels  \\\n",
       "0     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2     [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "3     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                 ...   \n",
       "1011  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "1012  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1013  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "1014  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1015  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                            head_labels  \n",
       "0     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "2     [[0, 1, 0, 1, -1], [0, 0, 0, 0, 0], [1, 0, -1,...  \n",
       "3     [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "4     [[0, 0, 1, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "...                                                 ...  \n",
       "1011  [[0, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 1, -1,...  \n",
       "1012  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1013  [[0, 0, 0, 0, -1], [0, 0, 0, 0, 1], [0, 0, -1,...  \n",
       "1014  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "1015  [[1, 0, 0, 0, -1], [0, 0, 0, 0, 0], [0, 0, -1,...  \n",
       "\n",
       "[1016 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------attempt of including all heads in one column--------------------\n",
    "labels = {'No Finding':0, 'Cardiomegaly':1,'Lung Opacity':2,'Edema':3,'Consolidation':4,\n",
    "          'Pneumonia':5,'Atelectasis':6,'Pneumothorax':7,'Pleural Effusion':8,'Fracture':9,'SupportDevices':10}\n",
    "def reorder(row,groups_index):\n",
    "#     print(row)\n",
    "    groups_values = []\n",
    "    for group in groups_index:\n",
    "        tmp =[]\n",
    "        for index in group:\n",
    "            if index==-1:\n",
    "                tmp.append(-1)\n",
    "            else:\n",
    "                tmp.append(row[index])\n",
    "        groups_values.append(tmp)\n",
    "#     print(groups_values)\n",
    "    return groups_values\n",
    "\n",
    "def group_heads(groups,df): # first attempt for each label one head\n",
    "    df['head_labels'] = df.apply(lambda row: reorder(row['labels'],groups), axis=1)\n",
    "    return df\n",
    "        \n",
    "def padding_heads(heads_index): #as heads does not have the same size, we pad them with -1\n",
    "    head_counts = [len(head) for head in heads_index]\n",
    "    lenmax = max(head_counts)\n",
    "#     print(lenmax)\n",
    "    padded_heads = []\n",
    "    for head in heads_index:\n",
    "        pad_n = lenmax - len(head)\n",
    "#         print(pad_n)\n",
    "        arr = np.pad(head, (0,pad_n), 'constant', constant_values=-1)\n",
    "        padded_heads.append(arr)\n",
    "    return padded_heads\n",
    "\n",
    "\n",
    "head_index1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "head_index2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "head_index3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [head_index1, head_index2, head_index3]\n",
    "\n",
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "\n",
    "padded_heads=padding_heads(heads_index)\n",
    "train_df = group_heads(padded_heads, train_df)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #------------attempt of one column per head-------------\n",
    "# def get_head(labels, head_index):\n",
    "#     head_values = []\n",
    "#     for index in head_index:\n",
    "#         head_values.append(labels[index])\n",
    "#     return head_values\n",
    "        \n",
    "# def split_data_to_heads(heads_index,df):\n",
    "#     i=1\n",
    "#     for head_index in heads_index:\n",
    "#         col_name = f\"head{i}\"\n",
    "#         df[col_name]= df.apply(lambda row: get_head(row['labels'],head_index), axis=1)\n",
    "#         i+=1\n",
    "#     return df\n",
    "    \n",
    "# train_df = split_data_to_heads(heads_index, train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head():\n",
    "    def __init__(self,hparams):\n",
    "        self.inputLayer = hparams['inputLayer']\n",
    "        self.run()\n",
    "    def run(self):\n",
    "        pass\n",
    "\n",
    "class HeadMultilabelCLS(Head):\n",
    "    def __init__(self,hparams):\n",
    "        self.labels = hparams['labels'] # batch labels, or all labels\n",
    "        self.device = hparams['device']\n",
    "        self.num_labels = hparams['num_labels'] # number of labels\n",
    "        self.taskspecificLayer = nn.Linear(hparams['input_size'], hparams['num_labels']).to(self.device) #classifier\n",
    "        self.loss = None\n",
    "        self.logits = None\n",
    "        self.pred_labels = None\n",
    "        \n",
    "        super().__init__(hparams)\n",
    "        \n",
    "    def run(self):\n",
    "        self.logits = self.taskspecificLayer(self.inputLayer)\n",
    "        self.pred_label = torch.sigmoid(self.logits)\n",
    "        \n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,self.num_labels),self.labels.type_as(logits).view(-1,self.num_labels)) #convert labels to float for calculation\n",
    "        # loss_func = BCELoss() \n",
    "        # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "        self.loss = loss\n",
    "        \n",
    "        return loss  \n",
    "    \n",
    "    \n",
    "# class HeadMulticlassCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         logits = self.taskspecificLayer(self.inputLayer)\n",
    "\n",
    "#         loss_fct = CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadBinaryCLS(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    \n",
    "# class HeadAbstractiveSumm(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "# class HeadRegression(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "        \n",
    "#     def run(self):\n",
    "#         loss_fct = MSELoss()\n",
    "#         loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "    \n",
    "    \n",
    "# class HeadSTSclinical(Head):\n",
    "#     def __init__(self,hparams):\n",
    "#         super().__init__(hparams)\n",
    "\n",
    "#     def run(self):\n",
    "#         pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertCLS: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertCLS from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertCLS from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "#         self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "#         self.head_count = config.head_count # ex:[2,3,4]\n",
    "#         self.head_index = config.head_index # ex: [[2,0],[1,3,4],[5,6,7,8,]]\n",
    "#         self.classifierheads = [nn.Linear(config.hidden_size, count) for count in self.head_count]\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "# [docs]\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "#     @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint=\"bert-base-uncased\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "#         #loop through the heads\n",
    "#         logit_heads = []\n",
    "#         for classifier in self.classifierheads:\n",
    "#             logits = classifier(pooled_output)\n",
    "#             logit_heads.append(logits)\n",
    "\n",
    "#         #TODO: work here on this for loop\n",
    "#         loss=0\n",
    "#         for logit,head_labels in zip(logit_heads):\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss += loss_func(logits.view(-1,head_num_labels),b_labels.type_as(logits).view(-1,head_num_labels)) #convert labels to float for calculation\n",
    "\n",
    "#         return loss  \n",
    "#-------------example------------------\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# import torch\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertCLS.from_pretrained('bert-base-uncased')\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# # labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(**inputs)\n",
    "# # loss, logits = outputs[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. fine tunning and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataLoader(input, labels, batch_size):\n",
    "    data = TensorDataset(input.input_ids, input.attention_mask, labels)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def calculate_f1_acc(pred_labels, true_labels, threshold = 0.50):\n",
    "    pred_bools = [pl>threshold for pl in pred_labels]\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "    val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "    val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "    print('Validation F1: ', val_f1_accuracy)\n",
    "    print('Validation Accuracy: ', val_flat_accuracy)\n",
    "\n",
    "\n",
    "def multihead_cls(data_path, split_path, PreTrainedModel, epochs, batch_size, max_length ,ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index):\n",
    "    #--------prepare the dataset-----------\n",
    "    train_df, test_df, val_df,  label_cols = read_data(data_path, split_path)\n",
    "    num_labels = len(label_cols)\n",
    "    \n",
    "    padded_heads=padding_heads(heads_index)\n",
    "    train_df = group_heads(padded_heads, train_df)\n",
    "    test_df = group_heads(padded_heads, test_df)\n",
    "    val_df = group_heads(padded_heads, val_df)\n",
    "    \n",
    "    # ----------tokenize---------------\n",
    "    tokenizer = ModelTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    reports_train = train_df.text.to_list()\n",
    "    reports_test = test_df.text.to_list()\n",
    "    reports_val   = val_df.text.to_list()\n",
    "\n",
    "    train = tokenizer(reports_train, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    test = tokenizer(reports_test, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val = tokenizer(reports_val, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    #-----------dataloaders--------------\n",
    "    #prepare labels for dataloader\n",
    "    head_count = [len(group) for group in heads_index]\n",
    "    nheads = len(head_count)\n",
    "    \n",
    "    train_labels = torch.from_numpy(np.array(train_df.head_labels.to_list()))\n",
    "    test_labels = torch.from_numpy(np.array(test_df.head_labels.to_list()))\n",
    "    val_labels = torch.from_numpy(np.array(val_df.head_labels.to_list()))\n",
    "\n",
    "    \n",
    "    if use_data_loader: # if the dataset is huge in size\n",
    "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "        # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "        train_dataloader      = create_dataLoader(train, train_labels, batch_size)\n",
    "        validation_dataloader = create_dataLoader(val, val_labels, batch_size)\n",
    "        test_dataloader       = create_dataLoader(test, test_labels, batch_size)\n",
    "\n",
    "    else: #TODO: if the dataset is small in size\n",
    "        pass\n",
    "    \n",
    "    #-----------load model----------------\n",
    "    # Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
    "    model = PreTrainedModel.from_pretrained(model_name)\n",
    "    model.cuda()\n",
    "    optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization\n",
    "    \n",
    "    #---------FineTune model-----------\n",
    "#     device = \"cpu\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # n_gpu = torch.cuda.device_count()\n",
    "    \n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "        #-------Training-------\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0 #running loss\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "#             print(b_labels)\n",
    "#             print(b_labels.size())\n",
    "#             print(b_labels[:,0,:])\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for multiclass classification\n",
    "            # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            # loss = outputs[0]\n",
    "            # logits = outputs[1]\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "#             outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "#             logits = outputs[0]\n",
    "#             loss_func = BCEWithLogitsLoss() \n",
    "#             loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             # loss_func = BCELoss() \n",
    "#             # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "#             train_loss_set.append(loss.item()) \n",
    "            \n",
    "            #Forward pass for multihead\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "#             print(outputs[0].size())\n",
    "            head_losses = []\n",
    "            for i in range(0,nheads):\n",
    "                \n",
    "                #remove -1 paddings:\n",
    "                labels = b_labels[:,i,:]\n",
    "                labels = labels[:,0:head_count[i]]\n",
    "                \n",
    "                hparams={\n",
    "                    'labels' : labels,\n",
    "                    'num_labels' : len(heads_index[i]),\n",
    "                    'input_size' : outputs[0].size()[0],\n",
    "                    'inputLayer' : outputs,\n",
    "                    'device'     : device,\n",
    "                }\n",
    "                head_losses.append(HeadMultilabelCLS(hparams))\n",
    "            \n",
    "            loss = 0\n",
    "            for head in head_losses:\n",
    "                loss += head.loss \n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "            \n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # ---------Validation--------\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Variables to gather full output\n",
    "        true_labels,pred_labels= [],[]\n",
    "\n",
    "        # Predict\n",
    "        for i, batch in enumerate(validation_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "\n",
    "                pred_label_heads = []\n",
    "                true_label_heads = []\n",
    "                for i in range(0,nheads):\n",
    "                    #remove -1 paddings:\n",
    "                    labels = b_labels[:,i,:]\n",
    "                    labels = labels[:,0:head_count[i]]\n",
    "\n",
    "                    hparams={\n",
    "                        'labels' : labels,\n",
    "                        'num_labels' : len(heads_index[i]),\n",
    "                        'input_size' : outputs[0].size()[0],\n",
    "                        'inputLayer' : outputs,\n",
    "                        'device'     : device,\n",
    "                    }\n",
    "                    pred_label_heads.append(HeadMultilabelCLS(hparams).pred_label)\n",
    "                    true_label_heads.append(labels)\n",
    "                \n",
    "                #store head labels\n",
    "                pred_label_b = pred_label_heads.to('cpu').numpy()\n",
    "                tue_labels_b = true_label_heads.to('cpu').numpy()\n",
    "\n",
    "            #store batch the labels\n",
    "            true_labels.append(tue_labels_b)\n",
    "            pred_labels.append(pred_label_b)\n",
    "\n",
    "        # Flatten outputs (****This should probably be different becuz of heads)\n",
    "        pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "        true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        \n",
    "        calculate_f1_acc(pred_labels, true_labels)\n",
    "\n",
    "#     # ---------test--------\n",
    "#     # Put model in evaluation mode to evaluate loss on the validation set\n",
    "#     model.eval()\n",
    "\n",
    "#     #track variables\n",
    "# #     logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "#     true_labels, pred_labels = [],[]\n",
    "#     # Predict\n",
    "#     for i, batch in enumerate(test_dataloader):\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         with torch.no_grad():\n",
    "#             # Forward pass\n",
    "# #             outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "# #             b_logit_pred = outs[0]\n",
    "# #             pred_label = torch.sigmoid(b_logit_pred)\n",
    "#             #Forward pass for multihead\n",
    "#             outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)  \n",
    "\n",
    "#             pred_label_heads = []\n",
    "#             for i in range(0,nheads):\n",
    "                \n",
    "#                 #remove -1 paddings:\n",
    "#                 labels = b_labels[:,i,:]\n",
    "#                 labels = labels[:,0:head_count[i]]\n",
    "                \n",
    "#                 hparams={\n",
    "#                     'labels' : labels,\n",
    "#                     'num_labels' : len(heads_index[i]),\n",
    "#                     'input_size' : outputs[0].size()[0],\n",
    "#                     'inputLayer' : outputs,\n",
    "#                     'device'     : device,\n",
    "#                 }\n",
    "#                 pred_label_heads.append(HeadMultilabelCLS(hparams).pred_label)\n",
    "                 \n",
    "                \n",
    "# #             b_logit_pred = b_logit_pred.detach().cpu().numpy() # detach is to avoid a copy\n",
    "#             pred_label = pred_label.to('cpu').numpy()\n",
    "#             b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "# #         tokenized_texts.append(b_input_ids)\n",
    "# #         logit_preds.append(b_logit_pred)\n",
    "#         true_labels.append(b_labels)\n",
    "#         pred_labels.append(pred_label)\n",
    "\n",
    "#     # Flatten outputs\n",
    "#     tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "#     pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "#     true_labels = [item for sublist in true_labels for item in sublist]\n",
    "#     # Converting flattened binary values to boolean values\n",
    "#     true_bools = [tl==1 for tl in true_labels]\n",
    "\n",
    "#     #We need to threshold our sigmoid function outputs which range from [0, 1]. Below I use 0.50 as a threshold.\n",
    "#     test_label_cols = label_cols\n",
    "#     pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
    "\n",
    "#     # Print and save classification report\n",
    "#     print(\"-----------test-----------\")\n",
    "#     print(\"Threshold: 0.5\")\n",
    "#     print('Test F1 Score: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "#     print('Test Accuracy: ', accuracy_score(true_bools, pred_bools))\n",
    "#     print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) ,'\\n')\n",
    "#     clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
    "#     # pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "#     print(clf_report)\n",
    "\n",
    "#     # Calculate Accuracy - maximize F1 accuracy by tuning threshold values. First with 'macro_thresholds' on the order of e^-1 then with 'micro_thresholds' on the order of e^-2\n",
    "#     print(\"-----Optimizing threshold value for micro F1 score-----\")\n",
    "\n",
    "#     macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "#     f1_results, flat_acc_results = [], []\n",
    "#     for th in macro_thresholds:\n",
    "#         pred_bools = [pl>th for pl in pred_labels]\n",
    "#         test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "#         test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "#         f1_results.append(test_f1_accuracy)\n",
    "#         flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "#     best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "#     micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "#     f1_results, flat_acc_results = [], []\n",
    "#     for th in micro_thresholds:\n",
    "#         pred_bools = [pl>th for pl in pred_labels]\n",
    "#         test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "#         test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "#         f1_results.append(test_f1_accuracy)\n",
    "#         flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "#     best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "#     # Printing and saving classification report\n",
    "#     print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "#     print('Test F1 Score: ', f1_results[best_f1_idx])\n",
    "#     print('Test Accuracy: ', flat_acc_results[best_f1_idx])\n",
    "#     print('LRAP: ', label_ranking_average_precision_score(true_labels, pred_labels) , '\\n')\n",
    "\n",
    "#     best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "#     clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "#     # pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "#     print(clf_report_optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertCLS: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertCLS from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertCLS from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.8056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.9317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6416, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7231, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7186, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.7607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.6519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.5899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Train loss: 2.123971523717046\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [362, 17664]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-924701d24c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m multihead_cls(data_path,split_path, PreTrainedModel, epochs, batch_size, max_length ,\n\u001b[0;32m---> 53\u001b[0;31m                ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-08e119b532c6>\u001b[0m in \u001b[0;36mmultihead_cls\u001b[0;34m(data_path, split_path, PreTrainedModel, epochs, batch_size, max_length, ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mpred_bools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mtrue_bools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mval_f1_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_bools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_bools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mval_flat_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_bools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [362, 17664]"
     ]
    }
   ],
   "source": [
    "#------------dataset details------------\n",
    "data_path = 'data/OpenI/OpenI_cheXpertLabels.csv'\n",
    "split_path = 'data/OpenI/cheXpertLabels'\n",
    "use_data_loader = True\n",
    "#--------------training details-----------\n",
    "epochs = 3 # Number of training epochs (authors recommend between 2 and 4)\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "#---------------saving results details----------\n",
    "experiment_name = \"bert\"\n",
    "#----------------bert---------------\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "\n",
    "# # model_name = \"bert-base-cased\"\n",
    "# # tokenizer_name = \"bert-base-cased\"\n",
    "\n",
    "ModelTokenizer = BertTokenizer\n",
    "PreTrainedModel = BertCLS #BertForMultiheadClassification\n",
    "\n",
    "#----------------BioBERT-v1.0----------\n",
    "# model_name = \"monologg/biobert_v1.0_pubmed_pmc\" # from hugginface model list\n",
    "# model_name = \"model_wieghts/biobert_v1.0_pubmed_pmc\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#----------------BioBERT-v1.1----------\n",
    "# model_name = \"model_wieghts/biobert_v1.1_pubmed\"\n",
    "# tokenizer_name = \"bert-base-cased\"\n",
    "# ModelTokenizer = BertTokenizer\n",
    "# PreTrainedModel = BertForSequenceClassification\n",
    "\n",
    "#------------roberta-------------\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# model_name = 'roberta-base'\n",
    "# tokenizer_name = 'roberta-base'\n",
    "# ModelTokenizer = RobertaTokenizer\n",
    "# PreTrainedModel = RobertaForSequenceClassification\n",
    "\n",
    "#------------albert-------------\n",
    "# from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "# model_name = 'albert-base-v1'\n",
    "# tokenizer_name = 'albert-base-v1'\n",
    "# ModelTokenizer = AlbertTokenizer\n",
    "# PreTrainedModel = AlbertForSequenceClassification\n",
    "\n",
    "#medical sense\n",
    "group1 = [0,1,2,6]  #{'No Finding', 'Cardiomegaly','Lung Opacity','Atelectasis'}\n",
    "group2 = [3,4,5,7,9] #{'Edema','Consolidation','Pneumonia', 'Pneumothorax','Fracture'}\n",
    "group3 = [8,10] #{'Pleural Effusion','SupportDevices',}\n",
    "heads_index = [group1, group2, group3]\n",
    "\n",
    "multihead_cls(data_path,split_path, PreTrainedModel, epochs, batch_size, max_length ,\n",
    "               ModelTokenizer, tokenizer_name, model_name, use_data_loader, heads_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "\n",
    "c = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "np.pad(a, (0,3), 'constant', constant_values=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':2, 'b':3, 'c':4}\n",
    "# def test(a)\n",
    "#     print(a)\n",
    "# test(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[ 1,  0,  0,  0, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 0,  1,  0,  1, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 0,  0,  1,  0, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 0,  0,  1,  1, -1],\n",
    "        [ 0,  0,  1,  0, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 0,  0,  1,  0, -1],\n",
    "        [ 0,  0,  0,  0, -1],\n",
    "        [ 0,  1,  0,  0, -1],\n",
    "        [ 1,  0,  0,  0, -1],\n",
    "        [ 0,  1,  0,  0, -1],\n",
    "        [ 0,  0,  0,  0, -1]]\n",
    "a = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
