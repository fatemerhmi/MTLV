{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total No. of rows: 2447\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileNo</th>\n",
       "      <th>COMPARISON</th>\n",
       "      <th>INDICATION</th>\n",
       "      <th>FINDINGS</th>\n",
       "      <th>IMPRESSION</th>\n",
       "      <th>expert_labels</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>SupportDevices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>881</td>\n",
       "      <td>None available.</td>\n",
       "      <td>XXXX-year-old XXXX with dyspnea.</td>\n",
       "      <td>The lungs are without focal air space opacity....</td>\n",
       "      <td>No acute cardiopulmonary abnormality.</td>\n",
       "      <td>['No Finding']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Back pain</td>\n",
       "      <td>Heart size and mediastinal contour are normal....</td>\n",
       "      <td>No acute cardiopulmonary process.</td>\n",
       "      <td>['No Finding']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The lungs are clear. Heart size is normal. No ...</td>\n",
       "      <td>Clear lungs. No acute cardiopulmonary abnormal...</td>\n",
       "      <td>['No Finding']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cardiomediastinal silhouette is normal. Pulmon...</td>\n",
       "      <td>No acute cardiopulmonary disease.</td>\n",
       "      <td>['No Finding']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pruritic.</td>\n",
       "      <td>Cardiac and mediastinal contours are within no...</td>\n",
       "      <td>No acute findings.</td>\n",
       "      <td>['No Finding']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fileNo       COMPARISON                        INDICATION  \\\n",
       "0     881  None available.  XXXX-year-old XXXX with dyspnea.   \n",
       "1    1734              NaN                         Back pain   \n",
       "2     306              NaN                               NaN   \n",
       "3    1951              NaN                               NaN   \n",
       "4    1005              NaN                         Pruritic.   \n",
       "\n",
       "                                            FINDINGS  \\\n",
       "0  The lungs are without focal air space opacity....   \n",
       "1  Heart size and mediastinal contour are normal....   \n",
       "2  The lungs are clear. Heart size is normal. No ...   \n",
       "3  Cardiomediastinal silhouette is normal. Pulmon...   \n",
       "4  Cardiac and mediastinal contours are within no...   \n",
       "\n",
       "                                          IMPRESSION   expert_labels  \\\n",
       "0              No acute cardiopulmonary abnormality.  ['No Finding']   \n",
       "1                  No acute cardiopulmonary process.  ['No Finding']   \n",
       "2  Clear lungs. No acute cardiopulmonary abnormal...  ['No Finding']   \n",
       "3                  No acute cardiopulmonary disease.  ['No Finding']   \n",
       "4                                 No acute findings.  ['No Finding']   \n",
       "\n",
       "   No Finding  Cardiomegaly  Lung Opacity  Edema  Consolidation  Pneumonia  \\\n",
       "0           1             0             0      0              0          0   \n",
       "1           1             0             0      0              0          0   \n",
       "2           1             0             0      0              0          0   \n",
       "3           1             0             0      0              0          0   \n",
       "4           1             0             0      0              0          0   \n",
       "\n",
       "   Atelectasis  Pneumothorax  Pleural Effusion  Fracture  SupportDevices  \n",
       "0            0             0                 0         0               0  \n",
       "1            0             0                 0         0               0  \n",
       "2            0             0                 0         0               0  \n",
       "3            0             0                 0         0               0  \n",
       "4            0             0                 0         0               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/OpenI/OpenI_cheXpertLabels.csv')\n",
    "print(\"The total No. of rows:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 1 per label: \n",
      " No Finding          1391\n",
      "Cardiomegaly         375\n",
      "Lung Opacity         406\n",
      "Edema                 46\n",
      "Consolidation         30\n",
      "Pneumonia             42\n",
      "Atelectasis          332\n",
      "Pneumothorax          23\n",
      "Pleural Effusion     161\n",
      "Fracture              84\n",
      "SupportDevices       134\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "labels = list(cols[6:])\n",
    "print('Count of 1 per label: \\n', df[labels].sum(), '\\n') # Label counts, may need to downsample or upsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. removing the rows without any Impression and Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows with Finding: 2100\n",
      "No. of rows with Impression: 2415\n",
      "No. of rows with Impression or Finding: 2419\n",
      "No. of rows without Impression and Finding: 28\n"
     ]
    }
   ],
   "source": [
    "print('No. of rows with Finding:', len(df[df['FINDINGS'].notnull()]))\n",
    "print('No. of rows with Impression:', len(df[df['IMPRESSION'].notnull()]))\n",
    "print('No. of rows with Impression or Finding:', \n",
    "      len(df[df['IMPRESSION'].notnull() | df['FINDINGS'].notnull()]))\n",
    "print('No. of rows without Impression and Finding:', \n",
    "      len(df[df['IMPRESSION'].isna() & df['FINDINGS'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows without Impression and Finding: 0\n"
     ]
    }
   ],
   "source": [
    "idx = df[df['IMPRESSION'].isna() & df['FINDINGS'].isna()].index\n",
    "df = df.drop(idx)\n",
    "print('No. of rows without Impression and Finding:', \n",
    "      len(df[df['IMPRESSION'].isna() & df['FINDINGS'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Converting the labels to a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The lungs are without focal air space opacity....</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart size and mediastinal contour are normal....</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The lungs are clear. Heart size is normal. No ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardiomediastinal silhouette is normal. Pulmon...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardiac and mediastinal contours are within no...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mediastinal contours are normal. Lungs are cle...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Heart size and mediastinal contours are unrema...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The heart is again enlarged, stable. The left ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Normal heart size. No focal air space consolid...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Heart size within normal limits. No focal airs...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The lungs are without focal air space opacity....   \n",
       "1  Heart size and mediastinal contour are normal....   \n",
       "2  The lungs are clear. Heart size is normal. No ...   \n",
       "3  Cardiomediastinal silhouette is normal. Pulmon...   \n",
       "4  Cardiac and mediastinal contours are within no...   \n",
       "5  Mediastinal contours are normal. Lungs are cle...   \n",
       "6  Heart size and mediastinal contours are unrema...   \n",
       "7  The heart is again enlarged, stable. The left ...   \n",
       "8  Normal heart size. No focal air space consolid...   \n",
       "9  Heart size within normal limits. No focal airs...   \n",
       "\n",
       "                              labels  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "6  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "8  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "9  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['No Finding', 'Cardiomegaly','Lung Opacity','Edema','Consolidation','Pneumonia','Atelectasis','Pneumothorax','Pleural Effusion','Fracture','SupportDevices']\n",
    "\n",
    "df_cls = pd.DataFrame(columns = ['text', 'labels'])\n",
    "\n",
    "def concat_cols(impression, findings):\n",
    "    if impression is np.nan:\n",
    "        return findings\n",
    "    elif findings is np.nan:\n",
    "        return impression\n",
    "    else:\n",
    "        return findings+impression\n",
    "\n",
    "# create the text column:\n",
    "df_cls['text'] = df.apply(lambda row: concat_cols(row['IMPRESSION'], row['FINDINGS']), axis=1)\n",
    "df_cls['labels'] = df.apply(lambda row: row[labels].to_list(), axis=1) #.to_list() , .values\n",
    "\n",
    "df_cls.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removing the duplicate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique texts:  False\n"
     ]
    }
   ],
   "source": [
    "print('Unique texts: ', df_cls.text.nunique() == df_cls.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of whole dataframe: 2419\n",
      "No. of unique reports: 1742\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of whole dataframe:\", len(df_cls))\n",
    "print(\"No. of unique reports:\", df_cls.text.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The heart is normal in size. The mediastinum is unremarkable. The lungs are clear.No acute disease.                                                                                                                                                                                                                                                                                                                     49\n",
       "Heart size normal. Lungs are clear. XXXX are normal. No pneumonia, effusions, edema, pneumothorax, adenopathy, nodules or masses.Normal chest                                                                                                                                                                                                                                                                           35\n",
       "Both lungs are clear and expanded. Heart and mediastinum normal.No active disease.                                                                                                                                                                                                                                                                                                                                      31\n",
       "The lungs are clear bilaterally. Specifically, no evidence of focal consolidation, pneumothorax, or pleural effusion.. Cardio mediastinal silhouette is unremarkable. Visualized osseous structures of the thorax are without acute abnormality.No acute cardiopulmonary abnormality..                                                                                                                                  31\n",
       "Lungs are clear bilaterally. Cardiac and mediastinal silhouettes are normal. Pulmonary vasculature is normal. No pneumothorax or pleural effusion. No acute bony abnormality.No acute cardiopulmonary abnormality.                                                                                                                                                                                                      21\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                        ..\n",
       "Heart size and mediastinal contour normal. Lungs are clear. Pulmonary vascularity normal. No pleural effusions or pneumothoraces. Minimal degenerative changes thoracic spine.No acute cardiopulmonary process.                                                                                                                                                                                                          1\n",
       "Cardiac silhouette is normal in size. Normal mediastinal contour and pulmonary vasculature. The lungs are without focal airspace consolidation, large pleural effusion, or pneumothoraces.No acute cardiopulmonary findings.                                                                                                                                                                                             1\n",
       "PA and lateral views. stable postoperative changes with midline sternotomy XXXX and myocardial revascularization. Cardiac size remains mildly enlarged but stable. There is mild vascular congestion. Small bilateral pleural effusions are present, which are XXXX.Mild pulmonary vascular congestion, with XXXX XXXX bilateral effusions. Constellation findings is most compatible with congestive heart failure.     1\n",
       "The heart is top normal in size. The mediastinum is Stable. The aorta is atherosclerotic. There are mild chronic changes without focal consolidation. No pleural effusion is seen.Mild cardiomegaly and atherosclerosis. No acute infiltrate.                                                                                                                                                                            1\n",
       "Normal heart size and mediastinal contour. Right lung base airspace disease on frontal XXXX. XXXX opacities in the left lung base consistent with atelectasis. No pneumothorax. No pleural effusion. Mild wedge XXXX deformity of T12.Right lung base airspace disease and left base atelectasis.                                                                                                                        1\n",
       "Name: text, Length: 1742, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at what are the duplicated texts looks like:\n",
    "df_cls.text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicates\n",
    "df_cls.drop_duplicates('text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique texts:  True\n"
     ]
    }
   ],
   "source": [
    "print('Unique texts: ', df_cls.text.nunique() == df_cls.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "**First Attempt:**   \n",
    "train_test_split from sklearn (stratified)   \n",
    "Failed    \n",
    "\n",
    "**Second Attempt**      \n",
    "http://scikit.ml/stratification.html     \n",
    "This didn't work, It only accepted their own type of input and didn't update the repo for the last two years.    \n",
    "Failed\n",
    "\n",
    "**Third Attempt**\n",
    "splitting a multilabel dataset(stratified)     \n",
    "https://vict0rs.ch/2018/05/24/sample-multilabel-dataset/    \n",
    "Passed (The code of Experiment 1 is using this attempt)\n",
    "\n",
    "**Forth Attempt**      \n",
    "Splitting in a random way using sklearn    \n",
    "Passed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------first attempt-------------(unsuccessful)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# trainX, valX, trainY, valY = train_test_split(df_cls['text'].values.tolist(), \n",
    "#                                               df_cls['labels'].values.tolist(), \n",
    "#                                               test_size=0.5,\n",
    "#                                               stratify = df_cls['labels'].values.tolist())\n",
    "# train, test = train_test_split(df, test_size=0.5, random_state=0, \n",
    "#                                stratify = df[labels])\n",
    "# ValueError: The least populated class in y has only 1 member, which is too few. \n",
    "# The minimum number of groups for any class cannot be less than 2.\n",
    "\n",
    "# This because of the nature of stratification. The stratify parameter set it \n",
    "# to split data in a way to allocate test_size amount of data to each class. \n",
    "# In this case, you don't have sufficient class labels of one of your classes \n",
    "# to keep the data splitting ratio equal to test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------second attempt--------- (unsuccessful)\n",
    "# https://github.com/scikit-multilearn/scikit-multilearn\n",
    "\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "# from skmultilearn.dataset import load_dataset\n",
    "# from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "# from collections import Counter\n",
    "\n",
    "# X,y, _, _ = load_dataset('scene', 'undivided')\n",
    "# Counter(combination for row in get_combination_wise_output_matrix(y.A, order=2) for combination in row)\n",
    "\n",
    "# X_train, y_train, X_test, y_test = iterative_train_test_split(df_cls['text'].values, df_cls['labels'].values, test_size = 0.3)\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     'train': Counter(str(combination) for row in get_combination_wise_output_matrix(y_train.A, order=2) for combination in row),\n",
    "#     'test' : Counter(str(combination) for row in get_combination_wise_output_matrix(y_test.A, order=2) for combination in row)\n",
    "# }).T.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle\n",
    "df_cls = df_cls.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------third attempt-------------\n",
    "from copy import deepcopy\n",
    "\n",
    "def stratify(data, classes, ratios, one_hot=False):\n",
    "    \"\"\"Stratifying procedure.\n",
    "\n",
    "    data is a list of lists: a list of labels, for each sample.\n",
    "        Each sample's labels should be ints, if they are one-hot encoded, use one_hot=True\n",
    "    \n",
    "    classes is the list of classes each label can take\n",
    "\n",
    "    ratios is a list, summing to 1, of how the dataset should be split\n",
    "\n",
    "    \"\"\"\n",
    "    # one-hot decoding\n",
    "    if one_hot:\n",
    "        temp = [[] for _ in range(len(data))]\n",
    "        indexes, values = np.where(np.array(data).astype(int) == 1)\n",
    "        for k, v in zip(indexes, values):\n",
    "            temp[k].append(v)\n",
    "        data = temp\n",
    "\n",
    "    # Organize data per label: for each label l, per_label_data[l] contains the list of samples\n",
    "    # in data which have this label\n",
    "    per_label_data = {c: set() for c in classes}\n",
    "    for i, d in enumerate(data):\n",
    "        for l in d:\n",
    "            per_label_data[l].add(i)\n",
    "\n",
    "    # number of samples\n",
    "    size = len(data)\n",
    "\n",
    "    # In order not to compute lengths each time, they are tracked here.\n",
    "    subset_sizes = [r * size for r in ratios]\n",
    "    target_subset_sizes = deepcopy(subset_sizes)\n",
    "    per_label_subset_sizes = {\n",
    "        c: [r * len(per_label_data[c]) for r in ratios]\n",
    "        for c in classes\n",
    "    }\n",
    "\n",
    "    # For each subset we want, the set of sample-ids which should end up in it\n",
    "    stratified_data_ids = [set() for _ in range(len(ratios))]\n",
    "\n",
    "    # For each sample in the data set\n",
    "    while size > 0:\n",
    "        # Compute |Di|\n",
    "        lengths = {\n",
    "            l: len(label_data)\n",
    "            for l, label_data in per_label_data.items()\n",
    "        }\n",
    "        try:\n",
    "            # Find label of smallest |Di|\n",
    "            label = min(\n",
    "                {k: v for k, v in lengths.items() if v > 0}, key=lengths.get\n",
    "            )\n",
    "        except ValueError:\n",
    "            # If the dictionary in `min` is empty we get a Value Error. \n",
    "            # This can happen if there are unlabeled samples.\n",
    "            # In this case, `size` would be > 0 but only samples without label would remain.\n",
    "            # \"No label\" could be a class in itself: it's up to you to format your data accordingly.\n",
    "            break\n",
    "        current_length = lengths[label]\n",
    "\n",
    "        # For each sample with label `label`\n",
    "        while per_label_data[label]:\n",
    "            # Select such a sample\n",
    "            current_id = per_label_data[label].pop()\n",
    "\n",
    "            subset_sizes_for_label = per_label_subset_sizes[label]\n",
    "            # Find argmax clj i.e. subset in greatest need of the current label\n",
    "            largest_subsets = np.argwhere(\n",
    "                subset_sizes_for_label == np.amax(subset_sizes_for_label)\n",
    "            ).flatten()\n",
    "\n",
    "            if len(largest_subsets) == 1:\n",
    "                subset = largest_subsets[0]\n",
    "            # If there is more than one such subset, find the one in greatest need\n",
    "            # of any label\n",
    "            else:\n",
    "                largest_subsets = np.argwhere(\n",
    "                    subset_sizes == np.amax(subset_sizes)\n",
    "                ).flatten()\n",
    "                if len(largest_subsets) == 1:\n",
    "                    subset = largest_subsets[0]\n",
    "                else:\n",
    "                    # If there is more than one such subset, choose at random\n",
    "                    subset = np.random.choice(largest_subsets)\n",
    "\n",
    "            # Store the sample's id in the selected subset\n",
    "            stratified_data_ids[subset].add(current_id)\n",
    "\n",
    "            # There is one fewer sample to distribute\n",
    "            size -= 1\n",
    "            # The selected subset needs one fewer sample\n",
    "            subset_sizes[subset] -= 1\n",
    "\n",
    "            # In the selected subset, there is one more example for each label\n",
    "            # the current sample has\n",
    "            for l in data[current_id]:\n",
    "                per_label_subset_sizes[l][subset] -= 1\n",
    "            \n",
    "            # Remove the sample from the dataset, meaning from all per_label dataset created\n",
    "            for l, label_data in per_label_data.items():\n",
    "                if current_id in label_data:\n",
    "                    label_data.remove(current_id)\n",
    "\n",
    "    # Create the stratified dataset as a list of subsets, each containing the orginal labels\n",
    "    stratified_data_ids = [sorted(strat) for strat in stratified_data_ids]\n",
    "    stratified_data = [\n",
    "        [data[i] for i in strat] for strat in stratified_data_ids\n",
    "    ]\n",
    "\n",
    "    # Return both the stratified indexes, to be used to sample the `features` associated with your labels\n",
    "    # And the stratified labels dataset\n",
    "    return stratified_data_ids, stratified_data\n",
    "stratified_data_ids, stratified_data =  stratify(data=df_cls['labels'].values, classes=[0,1], ratios=[0.6,0.2,0.2], one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1043\n",
      "Test:  350\n",
      "Val:  349\n"
     ]
    }
   ],
   "source": [
    "train_df = df_cls.iloc[stratified_data_ids[0],:]\n",
    "test_df = df_cls.iloc[stratified_data_ids[1],:]\n",
    "val_df = df_cls.iloc[stratified_data_ids[2],:]\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Train: ', len(train_df))\n",
    "print('Test: ', len(test_df))\n",
    "print('Val: ', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-------+------+-----+\n",
      "|    Pathology     | total | train | test | val |\n",
      "+------------------+-------+-------+------+-----+\n",
      "|    No Finding    |  689  |  397  | 140  | 152 |\n",
      "|   Cardiomegaly   |  375  |  240  |  73  |  62 |\n",
      "|   Lung Opacity   |  406  |  263  |  76  |  67 |\n",
      "|      Edema       |   46  |   28  |  8   |  10 |\n",
      "|  Consolidation   |   30  |   22  |  3   |  5  |\n",
      "|    Pneumonia     |   42  |   24  |  10  |  8  |\n",
      "|   Atelectasis    |  332  |  208  |  62  |  62 |\n",
      "|   Pneumothorax   |   23  |   12  |  6   |  5  |\n",
      "| Pleural Effusion |  160  |   93  |  34  |  33 |\n",
      "|     Fracture     |   84  |   56  |  14  |  14 |\n",
      "|  SupportDevices  |  132  |   69  |  33  |  30 |\n",
      "+------------------+-------+-------+------+-----+\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "label_counts_total = np.array(df_cls.labels.to_list()).sum(axis=0)\n",
    "label_counts_train = np.array(train_df.labels.to_list()).sum(axis=0)\n",
    "label_counts_test = np.array(test_df.labels.to_list()).sum(axis=0)\n",
    "label_counts_val = np.array(val_df.labels.to_list()).sum(axis=0)\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "pretty=PrettyTable()\n",
    "pretty.field_names = ['Pathology', 'total', 'train', 'test','val']\n",
    "for pathology, cnt_total, cnt_train, cnt_test, cnt_val in zip(label_cols,label_counts_total, label_counts_train, label_counts_test, label_counts_val):\n",
    "    pretty.add_row([pathology, cnt_total, cnt_train, cnt_test, cnt_val])\n",
    "print(pretty)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------4th attempt (not stratified)---------\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_val_df, test_df = train_test_split(df_cls, test_size=0.2)\n",
    "# train_df, val_df = train_test_split(train_val_df, test_size=0.2)\n",
    "\n",
    "# print('Train: ', len(train_df))\n",
    "# print('Test: ', len(test_df))\n",
    "# print('Val: ', len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save train, test, val csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('data/OpenI/cheXpertLabels/train.csv', index=False)\n",
    "test_df.to_csv('data/OpenI/cheXpertLabels/test.csv', index=False)\n",
    "val_df.to_csv('data/OpenI/cheXpertLabels/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simpletransformers implementation\n",
    "\n",
    "* MultiLabelClassificationModel has an additional `threshold` parameter with default value 0.5\n",
    "* MultiLabelClassificationModel takes in an additional optional argument pos_weight. This should be a list with the same length as the number of labels. This enables using different weights for each label when calculating loss during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from simpletransformers.classification import MultiLabelClassificationModel\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# # Create a MultiLabelClassificationModel\n",
    "# model = MultiLabelClassificationModel('roberta', 'roberta-base', num_labels=len(labels), args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 5})\n",
    "# # You can set class weights by using the optional weight argument\n",
    "# print(train_df.head())\n",
    "\n",
    "# # Train the model\n",
    "# model.train_model(train_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(test_df)\n",
    "# # print(result)\n",
    "# # print(model_outputs)\n",
    "\n",
    "# predictions, raw_outputs = model.predict(test_df['text'])\n",
    "# # print(predictions)\n",
    "# # print(raw_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = predictions\n",
    "# y_true = test_df['labels'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cms = multilabel_confusion_matrix(y_true, y_pred)\n",
    "# def plot_hm(label, conMat):\n",
    "\n",
    "#     df_cm = pd.DataFrame(conMat, range(2), range(2))\n",
    "#     # plt.figure(figsize=(10,7))\n",
    "#     sn.set(font_scale=1.2) # for label size\n",
    "#     sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}, cmap=\"YlGnBu\", fmt=\"d\") # font size\n",
    "#     plt.show()\n",
    "\n",
    "# for cm,label in zip(cms, labels):\n",
    "#     print(label)\n",
    "# #     print(cm)\n",
    "#     plot_hm(label, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face implementation - Multi label classification\n",
    "\n",
    "This code is inspired from\n",
    "\n",
    "\n",
    "post: https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1\n",
    "\n",
    "code: https://colab.research.google.com/github/rap12391/transformers_multilabel_toxic/blob/master/toxic_multilabel.ipynb#scrollTo=uorMX_zrnISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trachea is midline. The cardiomediastinal ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is hyperinflation of the lungs. A small ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The heart is normal in size and contour. There...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The heart, pulmonary XXXX and mediastinum are ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The lungs are clear. The heart pulmonary XXXX ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The trachea is midline. The cardiomediastinal ...   \n",
       "1  There is hyperinflation of the lungs. A small ...   \n",
       "2  The heart is normal in size and contour. There...   \n",
       "3  The heart, pulmonary XXXX and mediastinum are ...   \n",
       "4  The lungs are clear. The heart pulmonary XXXX ...   \n",
       "\n",
       "                              labels  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average report length:  41.17\n",
      "stdev report length:  23.01\n"
     ]
    }
   ],
   "source": [
    "print('average report length: ', round(df_cls.text.str.split().str.len().mean(),2))\n",
    "print('stdev report length: ', round(df_cls.text.str.split().str.len().std(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns:  ['No Finding', 'Cardiomegaly', 'Lung Opacity', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Fracture', 'SupportDevices']\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns\n",
    "label_cols = list(cols[6:])\n",
    "num_labels = len(label_cols)\n",
    "print('Label columns: ', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Finding: 689\n",
      "Cardiomegaly: 375\n",
      "Lung Opacity: 406\n",
      "Edema: 46\n",
      "Consolidation: 30\n",
      "Pneumonia: 42\n",
      "Atelectasis: 332\n",
      "Pneumothorax: 23\n",
      "Pleural Effusion: 160\n",
      "Fracture: 84\n",
      "SupportDevices: 132\n"
     ]
    }
   ],
   "source": [
    "label_counts = np.array(df_cls.labels.to_list()).sum(axis=0)\n",
    "for col, cnt in zip(label_cols, label_counts):\n",
    "    print(f'{col}: {cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1043\n",
      "Test:  350\n",
      "Val:  349\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', len(train_df))\n",
    "print('Test: ', len(test_df))\n",
    "print('Val: ', len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The input_ids are the indices corresponding to each token in our sentence.\n",
    "* We can now see what the attention_mask is all about: it points out which tokens the model should pay attention to and which ones it should not (because they represent padding in this case).\n",
    "* token_type_ids are for: they indicate to the model which part of the inputs correspond to the first sentence and which part corresponds to the second sentence. Note that token_type_ids are not required or handled by all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "reports_train = train_df.text.to_list()\n",
    "reports_test = test_df.text.to_list()\n",
    "reports_val   = val_df.text.to_list()\n",
    "\n",
    "train = tokenizer(reports_train, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "test = tokenizer(reports_test, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "val = tokenizer(reports_val, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "train_labels = torch.from_numpy(np.array(train_df.labels.to_list()))\n",
    "test_labels = torch.from_numpy(np.array(test_df.labels.to_list()))\n",
    "val_labels = torch.from_numpy(np.array(val_df.labels.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "use_data_loader = True\n",
    "if use_data_loader: # if the dataset is huge in size\n",
    "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "    # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "    train_data = TensorDataset(train.input_ids, train.attention_mask, train_labels, train.token_type_ids)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(val.input_ids, val.attention_mask, val_labels, val.token_type_ids)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_data = TensorDataset(test.input_ids, test.attention_mask, test_labels, test.token_type_ids)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "else: #if the dataset is small in size\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting custom optimization parameters. You may implement a scheduler here as well.\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'gamma', 'beta']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#      'weight_decay_rate': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#      'weight_decay_rate': 0.0}\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
    "optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.42790035406748456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|      | 1/3 [00:14<00:28, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  41.061946902654874\n",
      "Flat Validation Accuracy:  32.95128939828081\n",
      "Train loss: 0.26798246891209576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|   | 2/3 [00:28<00:14, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  75.43859649122807\n",
      "Flat Validation Accuracy:  61.60458452722063\n",
      "Train loss: 0.20401788525509112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 3/3 [00:42<00:00, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Validation Accuracy:  82.36775818639799\n",
      "Flat Validation Accuracy:  69.34097421203438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss = 0 #running loss\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Forward pass for multiclass classification\n",
    "    # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    # loss = outputs[0]\n",
    "    # logits = outputs[1]\n",
    "\n",
    "    # Forward pass for multilabel classification\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    loss_func = BCEWithLogitsLoss() \n",
    "    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "    # loss_func = BCELoss() \n",
    "    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "    train_loss_set.append(loss.item())    \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Variables to gather full output\n",
    "  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "  for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      b_logit_pred = outs[0]\n",
    "      pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "      pred_label = pred_label.to('cpu').numpy()\n",
    "      b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "  # Flatten outputs\n",
    "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "  true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  threshold = 0.50\n",
    "  pred_bools = [pl>threshold for pl in pred_labels]\n",
    "  true_bools = [tl==1 for tl in true_labels]\n",
    "  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "  print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "  print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "#track variables\n",
    "logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        b_logit_pred = outs[0]\n",
    "        pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        pred_label = pred_label.to('cpu').numpy()\n",
    "        b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "# Converting flattened binary values to boolean values\n",
    "true_bools = [tl==1 for tl in true_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to threshold our sigmoid function outputs which range from [0, 1]. Below I use 0.50 as a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score:  0.8095238095238095\n",
      "Test Accuracy:  0.6532951289398281 \n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Finding       0.96      0.96      0.96       140\n",
      "    Cardiomegaly       1.00      0.77      0.87        73\n",
      "    Lung Opacity       0.95      1.00      0.97        75\n",
      "           Edema       0.00      0.00      0.00         8\n",
      "   Consolidation       0.00      0.00      0.00         3\n",
      "       Pneumonia       0.00      0.00      0.00        10\n",
      "     Atelectasis       0.86      0.92      0.89        62\n",
      "    Pneumothorax       0.00      0.00      0.00         6\n",
      "Pleural Effusion       0.00      0.00      0.00        33\n",
      "        Fracture       0.00      0.00      0.00        14\n",
      "  SupportDevices       0.00      0.00      0.00        33\n",
      "\n",
      "       micro avg       0.95      0.71      0.81       457\n",
      "       macro avg       0.34      0.33      0.34       457\n",
      "    weighted avg       0.73      0.71      0.71       457\n",
      "     samples avg       0.81      0.75      0.77       457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/local/pkg/python/root-python-3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_label_cols = label_cols\n",
    "pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
    "\n",
    "# Print and save classification report\n",
    "print('Test F1 Score: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "print('Test Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\n",
    "clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
    "# pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing threshold value for micro F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold:  0.46\n",
      "Test F1 Score:  0.818407960199005\n",
      "Test Accuracy:  0.667621776504298 \n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Finding       0.96      0.97      0.97       140\n",
      "    Cardiomegaly       1.00      0.81      0.89        73\n",
      "    Lung Opacity       0.95      1.00      0.97        75\n",
      "           Edema       0.00      0.00      0.00         8\n",
      "   Consolidation       0.00      0.00      0.00         3\n",
      "       Pneumonia       0.00      0.00      0.00        10\n",
      "     Atelectasis       0.87      0.95      0.91        62\n",
      "    Pneumothorax       0.00      0.00      0.00         6\n",
      "Pleural Effusion       0.00      0.00      0.00        33\n",
      "        Fracture       0.00      0.00      0.00        14\n",
      "  SupportDevices       0.00      0.00      0.00        33\n",
      "\n",
      "       micro avg       0.95      0.72      0.82       457\n",
      "       macro avg       0.34      0.34      0.34       457\n",
      "    weighted avg       0.73      0.72      0.72       457\n",
      "     samples avg       0.82      0.77      0.78       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy - maximize F1 accuracy by tuning threshold values. First with 'macro_thresholds' on the order of e^-1 then with 'micro_thresholds' on the order of e^-2\n",
    "\n",
    "macro_thresholds = np.array(range(1,10))/10\n",
    "\n",
    "f1_results, flat_acc_results = [], []\n",
    "for th in macro_thresholds:\n",
    "  pred_bools = [pl>th for pl in pred_labels]\n",
    "  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "  f1_results.append(test_f1_accuracy)\n",
    "  flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "best_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n",
    "\n",
    "micro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n",
    "\n",
    "f1_results, flat_acc_results = [], []\n",
    "for th in micro_thresholds:\n",
    "  pred_bools = [pl>th for pl in pred_labels]\n",
    "  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n",
    "  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n",
    "  f1_results.append(test_f1_accuracy)\n",
    "  flat_acc_results.append(test_flat_accuracy)\n",
    "\n",
    "best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "# Printing and saving classification report\n",
    "print('Best Threshold: ', micro_thresholds[best_f1_idx])\n",
    "print('Test F1 Score: ', f1_results[best_f1_idx])\n",
    "print('Test Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n",
    "\n",
    "best_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\n",
    "clf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\n",
    "# pickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\n",
    "print(clf_report_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From Transformers: https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification\n",
    "# from torch.nn import BCEWithLogitsLoss\n",
    "# class BertForMultiLabelClassification(BertPreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.num_labels = config.num_labels\n",
    "\n",
    "#         self.bert = BertModel(config)\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "# [docs]\n",
    "#     @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING.format(\"(batch_size, sequence_length)\"))\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids=None,\n",
    "#         attention_mask=None,\n",
    "#         token_type_ids=None,\n",
    "#         position_ids=None,\n",
    "#         head_mask=None,\n",
    "#         inputs_embeds=None,\n",
    "#         labels=None,\n",
    "#     ):\n",
    "\n",
    "\n",
    "#         outputs = self.bert(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             head_mask=head_mask,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#         )\n",
    "\n",
    "#         pooled_output = outputs[1]\n",
    "\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "#         if labels is not None:\n",
    "#             if self.num_labels == 1:\n",
    "#                 #  We are doing regression\n",
    "#                 loss_fct = MSELoss()\n",
    "#                 loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "#             else:\n",
    "# #                 loss_fct = CrossEntropyLoss()\n",
    "#                 loss_fct = BCEWithLogitsLoss()\n",
    "#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#             outputs = (loss,) + outputs\n",
    "\n",
    "#         return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
