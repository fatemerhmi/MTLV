mlflow:
  experiment_name: ohsumed_experiments
  run_name:  MTL_KDE200_wlosspsum_e12
  tracking_uri: mlruns

dataset: 
  ohsumed:
    root: ./data
    data_split: True # True: train, test | False: train, test, val
    data_path: ohsumed
    use_data_loader: True

model: 
  bert:
    model_name: bert-base-uncased
    freeze: False
    
training:
  type: MTL_cls
  epoch : 12
  batch_size : 16
  use_cuda : True


head: 
  multi-task:
    heads: MultiLabelCLS
    type: KDE  # givenset meanshift KDE kmediod-label
    bandwidth: 200
    # elbow: 7
    # clusters: 4
    # count: 4
    # heads_index : [[0,1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19]]

tokenizer:
  bert:
    name: bert_base_uncased
    padding: max_length    # to pad to the longest sequence in the batch
    truncation: True  # truncate to a maximum length specified by the max_length
    max_length : 128

optimizer:
  adam:
    name: AdamW
    lr: 0.00002

loss:
  type: weighted_lossp_sum  #avgloss sumloss weighted_lossp_sum

# loss:
#   type: weightedsum # weightedavg weightedsum
#   weights: [0.45, 0.2, 0.15, 0.2]