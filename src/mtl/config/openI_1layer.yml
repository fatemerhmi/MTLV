experiment_name:
  OpenI_1layer

dataset: 
  openI:
    root: ./data
    data_split: True # True: train, test | False: train, test, val
    data_path: openI/OpenI_cheXpertLabels.csv
    split_path: openI/cheXpertLabels
    use_data_loader: True

model: 
  bert:
    model_name: bert-base-uncased
    
training:
  type: multihead_cls
  epoch : 3
  batch_size : 16
  use_cuda : True

head: 
  multi-head: #single or multi-task
    count: 3
    heads: MultiLabelCLS
    heads_index : [[0,1,2,6], [3,4,5,7,9], [8,10]]

tokenizer:
  bert:
    name: bert_base_uncased
    padding: max_length    # to pad to the longest sequence in the batch
    truncation: True  # truncate to a maximum length specified by the max_length
    max_length : 128

optimizer:
  name: AdamW
  lr: 0.00002
