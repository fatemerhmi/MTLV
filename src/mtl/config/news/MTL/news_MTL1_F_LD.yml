mlflow:
  experiment_name: news_experiments
  run_name: MTL_furthertraining_bert_kmedoidL_c3_sloss_e30_cv
  tracking_uri: mlruns

dataset: 
  news:
    root: ./data
    data_split: True # True: train, test | False: train, test, val
    data_path: news
    use_data_loader: True

model: 
  bert:
    model_name: bert-base-news #BioBERT-Basev1-1 #bert-base-uncased #BioBERT-Basev1-1 #BioBERT-Basev1-0-PM-PMC # #BioBERT-Basev1-0-PM-PMC # 
    freeze: False
    
training:
  type: MTL_cls
  epoch : 30
  batch_size : 16
  use_cuda : True
  cv : True #
  fold : 5

head: 
  multi-task:
    heads: MultiLabelCLS
    type: kmediod-labeldesc #kmediod-labeldesc #kmediod-labeldesc #kmediod-label givenset meanshift KDE kmediod-label, kmediod-labeldesc 
    # bandwidth: 20
    elbow: 8
    clusters: 3
    # count: 4
    # heads_index : [[0,1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19]]
    plot: true

tokenizer:
  bert:
    name:  bert-base-uncased #BioBERT-Basev1-1 # bert_base_uncased # bert_base_uncased
    padding: max_length    # to pad to the longest sequence in the batch
    truncation: True  # truncate to a maximum length specified by the max_length
    max_length : 256

optimizer:
  adam:
    name: AdamW
    lr: 0.00002

loss:
  type: sumloss #avgloss(aloss) sumloss(sloss) weighted_lossp_avg(wloss_a) weighted_lossp_sum(wloss_s)

# loss:
#   type: weightedsum # weightedavg weightedsum 
#   weights: [0.30, 0.30, 0.20, 0.20]
#   weights: [0.56, 0.15, 0.19, 0.10]

